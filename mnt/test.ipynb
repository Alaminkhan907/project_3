{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd1c36-77c7-4d65-aa2f-5a5bcb4965ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Concatenate title and abstract\n",
    "df = df.withColumn('combined_text', concat_ws(' ', df.title, df.abstract))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844b9f3-8fe5-417e-920e-c249652046da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"combined_text\", outputCol=\"tokens\")\n",
    "df_tokens = tokenizer.transform(df)\n",
    "\n",
    "# CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "cv_model = cv.fit(df_tokens)\n",
    "df_cv = cv_model.transform(df_tokens)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_cv)\n",
    "df_idf = idf_model.transform(df_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbb17c-3fcb-47f5-bfdf-3503d5b8f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import VectorUDT\n",
    "\n",
    "# Define UDF for converting sparse to dense vectors\n",
    "def to_dense(v):\n",
    "    if isinstance(v, SparseVector):\n",
    "        return DenseVector(v.toArray())\n",
    "    elif isinstance(v, DenseVector):\n",
    "        return v\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported vector type\")\n",
    "\n",
    "to_dense_udf = udf(to_dense, VectorUDT())\n",
    "\n",
    "# Apply UDF to convert features to dense vectors\n",
    "df_dense = df_idf.withColumn('features_dense', to_dense_udf(col('features')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7774ba-4bcb-40c6-a795-0274b37c4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(k=20, inputCol=\"features_dense\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_dense)\n",
    "df_pca = pca_model.transform(df_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9a19d-9672-4feb-b5e8-a88ff5a9f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for KMeans\n",
    "df_pca = df_pca.withColumn(\"pca_features_dense\", to_dense_udf(\"pca_features\"))\n",
    "\n",
    "# Elbow method to determine the best k\n",
    "costs = []\n",
    "k_values = range(2, 21)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features_dense\")\n",
    "    try:\n",
    "        model = kmeans.fit(df_pca)\n",
    "        costs.append(model.summary.trainingCost)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting KMeans with k={k}: {e}\")\n",
    "        break\n",
    "\n",
    "# Plot the Elbow Method results\n",
    "if costs:\n",
    "    plt.plot(k_values[:len(costs)], costs, marker='o')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid KMeans models were trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744090ba-b5fc-4af2-a291-25ac959addab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdebd14-881a-4428-9d9d-54b15717dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, VectorAssembler, PCA\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"TextFeatureExtraction\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [(\"This movie was absolutely fantastic!\",),\n",
    "        (\"I found this film to be quite disappointing.\",),\n",
    "        (\"The acting was superb, but the plot was weak.\",)]\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([StructField(\"review\", StringType(), True)])  \n",
    "\n",
    "# Create DataFrame with the schema\n",
    "df = spark.createDataFrame(data, schema=schema)  \n",
    "\n",
    "# 1. Tokenization ...making index of each word\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(df)\n",
    "# words_df.show()\n",
    "\n",
    "# 2. CountVectorizer (Term Frequency) ...How many time appears in the sentence\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "model = cv.fit(words_df)\n",
    "featurized_df = model.transform(words_df)\n",
    "# featurized_df.show()\n",
    "\n",
    "# 3. IDF (Inverse Document Frequency)  IDF(t)=log(1+DF(t)Nâ€‹) percentage of one sentence appear in one words\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_df)\n",
    "rescaled_df = idf_model.transform(featurized_df)\n",
    "# rescaled_df.show()\n",
    "\n",
    "# 4. VectorAssembler (Optional)\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"assembledFeatures\")\n",
    "assembled_df = assembler.transform(rescaled_df)\n",
    "\n",
    "# Show assembled features\n",
    "assembled_df.select(\"review\", \"assembledFeatures\").show(truncate=False)\n",
    "\n",
    "# 5. Apply PCA to reduce dimensions to 2\n",
    "pca = PCA(k=2, inputCol=\"assembledFeatures\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(assembled_df)\n",
    "pca_df = pca_model.transform(assembled_df)\n",
    "\n",
    "# Show the result\n",
    "pca_df.select(\"review\", \"pcaFeatures\").show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acffc4f-b025-40b4-859c-98423cb26e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"VectorAssemblerExample\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (0, 1.0, 3.0, 5.0),\n",
    "    (1, 2.0, 4.0, 6.0),\n",
    "    (2, 3.0, 5.0, 7.0)\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"id\", \"feature1\", \"feature2\", \"feature3\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Create VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "\n",
    "# Transform the DataFrame\n",
    "assembled_df = assembler.transform(df)\n",
    "\n",
    "# Show the result\n",
    "assembled_df.select(\"id\", \"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d697740-505b-4578-a3ca-9792d0ff36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PCAExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (0, Vectors.dense([2.5, 2.4])),\n",
    "    (1, Vectors.dense([0.5, 0.7])),\n",
    "    (2, Vectors.dense([2.2, 2.9])),\n",
    "    (3, Vectors.dense([1.9, 2.2])),\n",
    "    (4, Vectors.dense([3.1, 3.0])),\n",
    "    (5, Vectors.dense([2.3, 2.7])),\n",
    "    (6, Vectors.dense([2.0, 1.6])),\n",
    "    (7, Vectors.dense([1.0, 1.1])),\n",
    "    (8, Vectors.dense([1.5, 1.6])),\n",
    "    (9, Vectors.dense([1.1, 0.9]))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "# Show the result\n",
    "result.select(\"id\", \"features\", \"pcaFeatures\").show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b7bfc-44bb-4f18-9dda-2653b96f266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, VectorAssembler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"TextClustering\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [(\"This movie was absolutely fantastic!\",),\n",
    "        (\"I found this film to be quite disappointing.\",),\n",
    "        (\"The acting was superb, but the plot was weak.\",)]\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([StructField(\"review\", StringType(), True)])  \n",
    "\n",
    "# Create DataFrame with the schema\n",
    "df = spark.createDataFrame(data, schema=schema)  \n",
    "\n",
    "# 1. Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(df)\n",
    "\n",
    "# 2. CountVectorizer (Term Frequency)\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "model = cv.fit(words_df)\n",
    "featurized_df = model.transform(words_df)\n",
    "\n",
    "# 3. IDF (Inverse Document Frequency)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_df)\n",
    "rescaled_df = idf_model.transform(featurized_df)\n",
    "\n",
    "# 4. VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"assembledFeatures\")\n",
    "assembled_df = assembler.transform(rescaled_df)\n",
    "\n",
    "# 5. Apply PCA to keep 95% of the variance\n",
    "pca = PCA(k=2, inputCol=\"assembledFeatures\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(assembled_df)\n",
    "pca_df = pca_model.transform(assembled_df)\n",
    "\n",
    "# Show PCA-transformed features\n",
    "pca_df.select(\"review\", \"pcaFeatures\").show(truncate=False)\n",
    "\n",
    "# Elbow Method to determine the optimal number of clusters\n",
    "cost = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol='pcaFeatures')\n",
    "    model = kmeans.fit(pca_df)\n",
    "    predictions = model.transform(pca_df)\n",
    "    evaluator = ClusteringEvaluator(featuresCol='pcaFeatures')\n",
    "    cost.append(model.summary.trainingCost)\n",
    "\n",
    "# Plot the cost to visualize the elbow\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 10), cost, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal k (you can observe the elbow plot and decide)\n",
    "optimal_k = 3\n",
    "\n",
    "# Apply K-means clustering with the optimal k\n",
    "kmeans = KMeans(k=optimal_k, seed=1, featuresCol='pcaFeatures')\n",
    "kmeans_model = kmeans.fit(pca_df)\n",
    "kmeans_predictions = kmeans_model.transform(pca_df)\n",
    "\n",
    "# Show the result with cluster assignments\n",
    "kmeans_predictions.select(\"review\", \"pcaFeatures\", \"prediction\").show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a25f74e-6c61-4db0-bfe1-8fc66b350012",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper A: This study is about machine learning techniques\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m     12\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper B: A review on deep learning approaches\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m     13\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper C: An introduction to neural networks\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m     14\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper D: Advances in computer vision\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m     15\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper E: A study on reinforcement learning\u001b[39m\u001b[38;5;124m\"\u001b[39m,)]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define the schema explicitly\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mStructType\u001b[49m([StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)])  \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create DataFrame with the schema\u001b[39;00m\n\u001b[1;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, schema\u001b[38;5;241m=\u001b[39mschema)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, VectorAssembler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, array, lit, expr\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PaperRecommendation\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [(\"Paper A: This study is about machine learning techniques\",),\n",
    "        (\"Paper B: A review on deep learning approaches\",),\n",
    "        (\"Paper C: An introduction to neural networks\",),\n",
    "        (\"Paper D: Advances in computer vision\",),\n",
    "        (\"Paper E: A study on reinforcement learning\",)]\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([StructField(\"title\", StringType(), True)])  \n",
    "\n",
    "# Create DataFrame with the schema\n",
    "df = spark.createDataFrame(data, schema=schema)  \n",
    "\n",
    "# 1. Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(df)\n",
    "\n",
    "# 2. CountVectorizer (Term Frequency)\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "model = cv.fit(words_df)\n",
    "featurized_df = model.transform(words_df)\n",
    "\n",
    "# 3. IDF (Inverse Document Frequency)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_df)\n",
    "rescaled_df = idf_model.transform(featurized_df)\n",
    "\n",
    "# 4. VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"assembledFeatures\")\n",
    "assembled_df = assembler.transform(rescaled_df)\n",
    "\n",
    "# 5. Apply PCA to keep 95% of the variance\n",
    "pca = PCA(k=2, inputCol=\"assembledFeatures\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(assembled_df)\n",
    "pca_df = pca_model.transform(assembled_df)\n",
    "\n",
    "# 6. Apply K-means clustering with optimal k\n",
    "optimal_k = 2  # Assuming k=2 for simplicity\n",
    "kmeans = KMeans(k=optimal_k, seed=1, featuresCol='pcaFeatures')\n",
    "kmeans_model = kmeans.fit(pca_df)\n",
    "kmeans_predictions = kmeans_model.transform(pca_df)\n",
    "\n",
    "# Show the clusters\n",
    "kmeans_predictions.select(\"title\", \"pcaFeatures\", \"prediction\").show(truncate=False)\n",
    "\n",
    "# Define a function to calculate cosine similarity using SQL expressions\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = sum([v1[i] * v2[i] for i in range(len(v1))])\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Recommender function\n",
    "def recommend_papers(input_title, top_n):\n",
    "    # Find the cluster of the input title\n",
    "    input_paper = kmeans_predictions.filter(kmeans_predictions.title == input_title).select(\"pcaFeatures\", \"prediction\").first()\n",
    "    input_cluster = input_paper[\"prediction\"]\n",
    "    input_vector = input_paper[\"pcaFeatures\"]\n",
    "\n",
    "    # Filter papers in the same cluster\n",
    "    cluster_papers = kmeans_predictions.filter(kmeans_predictions.prediction == input_cluster).select(\"title\", \"pcaFeatures\")\n",
    "\n",
    "    # Compute cosine similarity with all papers in the same cluster\n",
    "    similarities = cluster_papers.withColumn(\"similarity\", expr(f\"vector_dot(pcaFeatures, array({', '.join(map(str, input_vector.toArray()))})) / (vector_norm(pcaFeatures) * vector_norm(array({', '.join(map(str, input_vector.toArray()))})))\"))\n",
    "\n",
    "    # Get top N recommendations\n",
    "    recommendations = similarities.orderBy(col(\"similarity\").desc()).limit(top_n)\n",
    "    \n",
    "    return recommendations.select(\"title\", \"similarity\").collect()\n",
    "\n",
    "# Example usage: Recommend top 3 papers similar to \"Paper A\"\n",
    "input_title = \"Paper A: This study is about machine learning techniques\"\n",
    "top_n = 1\n",
    "recommendations = recommend_papers(input_title, top_n)\n",
    "\n",
    "# Show recommendations\n",
    "for rec in recommendations:\n",
    "    print(f\"Title: {rec['title']}, Similarity: {rec['similarity']}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b437da0-0610-47d1-9255-85dac0cddf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar word: superb,, Similarity: 0.19690561294555664\n",
      "Similar word: fantastic!, Similarity: 0.0834183618426323\n",
      "Similar word: film, Similarity: 0.07623938471078873\n",
      "Similar word: disappointing., Similarity: 0.06888052821159363\n",
      "Similar word: quite, Similarity: 0.06005062907934189\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Word2VecExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [(\"This movie was absolutely fantastic!\",),\n",
    "        (\"I found this film to be quite disappointing.\",),\n",
    "        (\"The acting was superb, but the plot was weak.\",)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"words\", outputCol=\"word2vec_features\")\n",
    "model = word2vec.fit(df_tokenized)\n",
    "result = model.transform(df_tokenized)\n",
    "\n",
    "# Given word\n",
    "given_word = \"movie\"\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.findSynonyms(given_word, 5)  # Find top 5 similar words\n",
    "\n",
    "# Convert DataFrame to Pandas DataFrame\n",
    "similar_words_df = similar_words.toPandas()\n",
    "\n",
    "# Display the similar words\n",
    "for index, row in similar_words_df.iterrows():\n",
    "    print(f\"Similar word: {row['word']}, Similarity: {row['similarity']}\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7d477b-47d1-4a56-a0ba-f1df6298636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|filtered_words|   word2vec_features|\n",
      "+---+--------------+--------------------+\n",
      "|  0|     [a, b, c]|[-0.0025148376201...|\n",
      "|  1|     [d, e, f]|[0.00124437361955...|\n",
      "|  2|     [g, h, i]|[-0.0368569927910...|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Start Spark session with increased timeout settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Word2VecExample\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100000s\") \\\n",
    "    .config(\"spark.network.timeout\", \"10000000s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with filtered words\n",
    "df_filtered = spark.createDataFrame([\n",
    "    (0, [\"a\", \"b\", \"c\"]),\n",
    "    (1, [\"d\", \"e\", \"f\"]),\n",
    "    (2, [\"g\", \"h\", \"i\"])\n",
    "], [\"id\", \"filtered_words\"])\n",
    "\n",
    "# Train Word2Vec model\n",
    "# word2vec = Word2Vec(vectorSize=10, minCount=1, inputCol=\"filtered_words\", outputCol=\"word2vec_features\")\n",
    "\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=10, minCount=1, inputCol=\"filtered_words\", outputCol=\"word2vec_features\")\n",
    "model = word2vec.fit(df_filtered)\n",
    "df_vectorized = model.transform(df_filtered)\n",
    "\n",
    "try:\n",
    "    model = word2vec.fit(df_filtered)\n",
    "    df_vectorized = model.transform(df_filtered)\n",
    "    df_vectorized.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13f8eb-6ee2-4000-9a0b-4f9e00375572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
