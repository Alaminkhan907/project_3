{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd1c36-77c7-4d65-aa2f-5a5bcb4965ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, regexp_replace, lower\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, PCA, Word2Vec\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"BD_Analytics_Clustering_Project\").getOrCreate()\n",
    "\n",
    "# Step 1: Read the dataset\n",
    "file_path = \"/path/to/DBLP_Citation_network_V10.json\"\n",
    "df = spark.read.json(file_path)\n",
    "\n",
    "# Display schema and basic stats\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Step 2: Exploratory Data Analysis\n",
    "df.select(\"title\", \"abstract\").describe().show()\n",
    "\n",
    "# Step 3: Keep only English documents\n",
    "# (Assuming there's a language field, if not, additional language detection logic is needed)\n",
    "df = df.filter(df.language == \"English\")\n",
    "\n",
    "# Step 4: Preprocessing\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', \n",
    "                     'author', 'figure','rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "\n",
    "# User-defined function to remove custom stop words\n",
    "def remove_custom_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in custom_stop_words]\n",
    "\n",
    "remove_custom_stop_words_udf = udf(remove_custom_stop_words, ArrayType(StringType()))\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df = remover.transform(df)\n",
    "\n",
    "# Remove custom stop words\n",
    "df = df.withColumn(\"filtered_words\", remove_custom_stop_words_udf(col(\"filtered_words\")))\n",
    "\n",
    "# Remove punctuation and convert to lower case\n",
    "regex_pattern = re.compile('[!()-[\\]{};:\"\\\\,<>./?@#$%^&*_~]')\n",
    "df = df.withColumn(\"cleaned_words\", regexp_replace(col(\"filtered_words\"), regex_pattern, ''))\n",
    "df = df.withColumn(\"cleaned_words\", lower(col(\"cleaned_words\")))\n",
    "\n",
    "# Step 5: Vectorization\n",
    "# TF-IDF\n",
    "vectorizer = CountVectorizer(inputCol=\"cleaned_words\", outputCol=\"raw_features\")\n",
    "vectorizer_model = vectorizer.fit(df)\n",
    "df = vectorizer_model.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "# Word2Vec (optional, uncomment if needed)\n",
    "# word2Vec = Word2Vec(inputCol=\"cleaned_words\", outputCol=\"word2vec_features\")\n",
    "# model = word2Vec.fit(df)\n",
    "# df = model.transform(df)\n",
    "\n",
    "# Step 6: Clustering\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(k=20, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df)\n",
    "df = pca_model.transform(df)\n",
    "\n",
    "# Extract values array from pca_features and filter rows\n",
    "df = df.withColumn(\"values\", col(\"pca_features\").values)\n",
    "df = df.filter(size(col(\"values\")) == 20)\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(k=5, seed=1, featuresCol=\"pca_features\")\n",
    "model = kmeans.fit(df)\n",
    "df = model.transform(df)\n",
    "\n",
    "# Step 7: Search engine\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return float(vec1.dot(vec2) / (Vectors.norm(vec1) * Vectors.norm(vec2)))\n",
    "\n",
    "def recommend_papers(title, N):\n",
    "    # Find the paper\n",
    "    target_paper = df.filter(df.title == title).select(\"features\").collect()[0].features\n",
    "    # Calculate similarity\n",
    "    similarity_udf = udf(lambda x: cosine_similarity(x, target_paper), FloatType())\n",
    "    df_with_similarity = df.withColumn(\"similarity\", similarity_udf(col(\"features\")))\n",
    "    # Get top N similar papers\n",
    "    recommendations = df_with_similarity.orderBy(col(\"similarity\").desc()).select(\"title\").limit(N)\n",
    "    return recommendations\n",
    "\n",
    "# Example usage:\n",
    "recommendations = recommend_papers(\"Example Title\", 5)\n",
    "recommendations.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
