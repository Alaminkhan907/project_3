{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80f0397-4b96-41c0-8f3d-1811da45ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|Based on biologic...|[Guoping Pang, La...|4aa69add-3978-480...|         8|[04754a28-6bf4-4d...|Dynamic analysis ...|Mathematics and C...|2008|\n",
      "|In this paper, a ...|[S. Ben Jabra, Ez...|4ab3735c-80f1-472...|        50|[09cb2d7d-47d1-4a...|A new approach of...|international sym...|2008|\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|AdaBoost algorith...|[Zheng Xu, Runbin...|001eef4f-1d00-4ae...|         0|[0a11984c-ab6e-4b...|A Heterogeneous S...|high performance ...|2016|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "\n",
      "Number of records: 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DBLP Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)\n",
    "\n",
    "# Show the schema to understand the structure\n",
    "df.printSchema()\n",
    "\n",
    "# Display a sample of the data\n",
    "df.show(5)\n",
    "\n",
    "# Count the number of records\n",
    "print(f\"Number of records: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372fbb58-fee5-4692-bc57-56b283ccacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|summary|            abstract|                  id|       n_citation|               title|               venue|              year|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|  count|                   4|                   4|                4|                   4|                   4|                 4|\n",
      "|   mean|                NULL|                NULL|             14.5|                NULL|                NULL|           2011.25|\n",
      "| stddev|                NULL|                NULL|23.96525262402492|                NULL|                NULL|3.9475730941089733|\n",
      "|    min|AdaBoost algorith...|00127ee2-cb05-48c...|                0|A Heterogeneous S...|Mathematics and C...|              2008|\n",
      "|    max|The purpose of th...|4ab3735c-80f1-472...|               50|Preliminary Desig...|international sym...|              2016|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|       0|      0|  0|         0|         0|    0|    0|   0|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|       n_citation|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|             14.5|\n",
      "| stddev|23.96525262402492|\n",
      "|    min|                0|\n",
      "|    max|               50|\n",
      "+-------+-----------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Check for missing values (excluding `isnan`)\n",
    "from pyspark.sql import functions as F\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Distribution of citations\n",
    "df.select(\"n_citation\").describe().show()\n",
    "\n",
    "# Check for null abstracts and titles\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea27bebd-7662-4d10-b1a1-7ed5992ebd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langid\n",
      "  Using cached langid-1.1.6-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from langid) (1.26.4)\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install langid\n",
    "import langid\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to detect language using langid\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Add a new column for language detection\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "\n",
    "# Filter only English documents\n",
    "df = df.filter(df.language == 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7782509-85f0-4bfa-b99a-13c210279cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |cleaned_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |tokenized_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |filtered_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Based on biological control strategy in pest management, we construct and investigate a pest-epidemic model with impulsive control, i.e., periodic spraying microbial pesticide and releasing infected pests at different fixed moments. By using Floquet theorem and comparison theorem, we prove that the pest-eradication periodic solution is globally asymptotically stable when the impulsive period @t is less than the critical value @t\"m\"a\"x\"@?. Otherwise, the system can be permanent. Moreover, numerical results clearly show with the increase of the impulsive period @t, the system exhibits a wide variety of dynamic behaviors including a sequence of direct and inverse cascade of periodic-doubling, symmetry-breaking pitchfork bifurcation, chaos and non-unique dynamics, which implies that the impulsive effect makes the dynamic behavior of the system more complex.                                                                                                                                                                                                                                                                                                                                |based on biological control strategy in pest management we construct and investigate a pestepidemic model with impulsive control ie periodic spraying microbial pesticide and releasing infected pests at different fixed moments by using floquet theorem and comparison theorem we prove that the pesteradication periodic solution is globally asymptotically stable when the impulsive period t is less than the critical value tmax otherwise the system can be permanent moreover numerical results clearly show with the increase of the impulsive period t the system exhibits a wide variety of dynamic behaviors including a sequence of direct and inverse cascade of periodicdoubling symmetrybreaking pitchfork bifurcation chaos and nonunique dynamics which implies that the impulsive effect makes the dynamic behavior of the system more complex                                                                                                                                                                                                                                                                                                                                  |[based, on, biological, control, strategy, in, pest, management, we, construct, and, investigate, a, pestepidemic, model, with, impulsive, control, ie, periodic, spraying, microbial, pesticide, and, releasing, infected, pests, at, different, fixed, moments, by, using, floquet, theorem, and, comparison, theorem, we, prove, that, the, pesteradication, periodic, solution, is, globally, asymptotically, stable, when, the, impulsive, period, t, is, less, than, the, critical, value, tmax, otherwise, the, system, can, be, permanent, moreover, numerical, results, clearly, show, with, the, increase, of, the, impulsive, period, t, the, system, exhibits, a, wide, variety, of, dynamic, behaviors, including, a, sequence, of, direct, and, inverse, cascade, of, periodicdoubling, symmetrybreaking, pitchfork, bifurcation, chaos, and, nonunique, dynamics, which, implies, that, the, impulsive, effect, makes, the, dynamic, behavior, of, the, system, more, complex]                                                                                                                                                                                                                                                                                                                                                                           |[based, biological, control, strategy, pest, management, construct, investigate, pestepidemic, model, impulsive, control, ie, periodic, spraying, microbial, pesticide, releasing, infected, pests, different, fixed, moments, floquet, theorem, comparison, theorem, prove, pesteradication, periodic, solution, globally, asymptotically, stable, impulsive, period, less, critical, value, tmax, otherwise, system, permanent, moreover, numerical, results, clearly, show, increase, impulsive, period, system, exhibits, wide, variety, dynamic, behaviors, including, sequence, direct, inverse, cascade, periodicdoubling, symmetrybreaking, pitchfork, bifurcation, chaos, nonunique, dynamics, implies, impulsive, effect, makes, dynamic, behavior, system, complex]                                                                                                                                                                                                                                                                                                                             |\n",
      "|In this paper, a robust 3D triangular mesh watermarking algorithm based on 3D segmentation is proposed. In this algorithm three classes of watermarking are combined. First, we segment the original image to many different regions. Then we mark every type of region with the corresponding algorithm based on their curvature value. The experiments show that our watermarking is robust against numerous attacks including RST transformations, smoothing, additive random noise, cropping, simplification and remeshing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |in this paper a robust 3d triangular mesh watermarking algorithm based on 3d segmentation is proposed in this algorithm three classes of watermarking are combined first we segment the original image to many different regions then we mark every type of region with the corresponding algorithm based on their curvature value the experiments show that our watermarking is robust against numerous attacks including rst transformations smoothing additive random noise cropping simplification and remeshing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |[in, this, paper, a, robust, 3d, triangular, mesh, watermarking, algorithm, based, on, 3d, segmentation, is, proposed, in, this, algorithm, three, classes, of, watermarking, are, combined, first, we, segment, the, original, image, to, many, different, regions, then, we, mark, every, type, of, region, with, the, corresponding, algorithm, based, on, their, curvature, value, the, experiments, show, that, our, watermarking, is, robust, against, numerous, attacks, including, rst, transformations, smoothing, additive, random, noise, cropping, simplification, and, remeshing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |[paper, robust, 3d, triangular, mesh, watermarking, algorithm, based, 3d, segmentation, proposed, algorithm, three, classes, watermarking, combined, first, segment, original, image, many, different, regions, mark, every, type, region, corresponding, algorithm, based, curvature, value, experiments, show, watermarking, robust, numerous, attacks, including, rst, transformations, smoothing, additive, random, noise, cropping, simplification, remeshing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|The purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net- works. More specifically, we focus on the basic principles of network proto- cols as the aim to develop our learning tool. Our tool gives students hands-on experience to help understand the basic principles of network protocols.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |the purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net works more specifically we focus on the basic principles of network proto cols as the aim to develop our learning tool our tool gives students handson experience to help understand the basic principles of network protocols                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |[the, purpose, of, this, study, is, to, develop, a, learning, tool, for, high, school, students, studying, the, scientific, aspects, of, information, and, communication, net, works, more, specifically, we, focus, on, the, basic, principles, of, network, proto, cols, as, the, aim, to, develop, our, learning, tool, our, tool, gives, students, handson, experience, to, help, understand, the, basic, principles, of, network, protocols]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |[purpose, study, develop, learning, tool, high, school, students, studying, scientific, aspects, information, communication, net, works, specifically, focus, basic, principles, network, proto, cols, aim, develop, learning, tool, tool, gives, students, handson, experience, help, understand, basic, principles, network, protocols]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|AdaBoost algorithm based on Haar-like features can achieves high accuracy (above 95%) in object detection. Meanwhile massive computing power is needed to implement the cascaded classifiers involved in AdaBoost detection. To solve this problem, several dedicated hardware solutions have been proposed for real-time applications. In this work, a novel heterogeneous architecture of an AdaBoost detector is presented. This architecture achieves higher performance while consuming fewer hardware resources. By combining an integrated ARM Cortex-A9 processor with a dedicated accelerator, this architecture can be configured to realize various objects detection by simply loading different parameters. 2-D parallelism is involved in accelerator unit combination which brings more flexibility. This scheme is implemented on Xilinx ZC702 platform, the experiment result shows that 40 QVGA frames per second can be achieved for real-time face detection. The accelerator achieves more than 13 times improvement over the OpenCV implementation on a standalone Cortex-A9 w.r.t execution speed. Meanwhile, the accelerator consumes 40% less FPGA hardware resources than the prior-art implementation.|adaboost algorithm based on haarlike features can achieves high accuracy above 95 in object detection meanwhile massive computing power is needed to implement the cascaded classifiers involved in adaboost detection to solve this problem several dedicated hardware solutions have been proposed for realtime applications in this work a novel heterogeneous architecture of an adaboost detector is presented this architecture achieves higher performance while consuming fewer hardware resources by combining an integrated arm cortexa9 processor with a dedicated accelerator this architecture can be configured to realize various objects detection by simply loading different parameters 2d parallelism is involved in accelerator unit combination which brings more flexibility this scheme is implemented on xilinx zc702 platform the experiment result shows that 40 qvga frames per second can be achieved for realtime face detection the accelerator achieves more than 13 times improvement over the opencv implementation on a standalone cortexa9 wrt execution speed meanwhile the accelerator consumes 40 less fpga hardware resources than the priorart implementation|[adaboost, algorithm, based, on, haarlike, features, can, achieves, high, accuracy, above, 95, in, object, detection, meanwhile, massive, computing, power, is, needed, to, implement, the, cascaded, classifiers, involved, in, adaboost, detection, to, solve, this, problem, several, dedicated, hardware, solutions, have, been, proposed, for, realtime, applications, in, this, work, a, novel, heterogeneous, architecture, of, an, adaboost, detector, is, presented, this, architecture, achieves, higher, performance, while, consuming, fewer, hardware, resources, by, combining, an, integrated, arm, cortexa9, processor, with, a, dedicated, accelerator, this, architecture, can, be, configured, to, realize, various, objects, detection, by, simply, loading, different, parameters, 2d, parallelism, is, involved, in, accelerator, unit, combination, which, brings, more, flexibility, this, scheme, is, implemented, on, xilinx, zc702, platform, the, experiment, result, shows, that, 40, qvga, frames, per, second, can, be, achieved, for, realtime, face, detection, the, accelerator, achieves, more, than, 13, times, improvement, over, the, opencv, implementation, on, a, standalone, cortexa9, wrt, execution, speed, meanwhile, the, accelerator, consumes, 40, less, fpga, hardware, resources, than, the, priorart, implementation]|[adaboost, algorithm, based, haarlike, features, achieves, high, accuracy, 95, object, detection, meanwhile, massive, computing, power, needed, implement, cascaded, classifiers, involved, adaboost, detection, solve, problem, several, dedicated, hardware, solutions, proposed, realtime, applications, work, novel, heterogeneous, architecture, adaboost, detector, presented, architecture, achieves, higher, performance, consuming, fewer, hardware, resources, combining, integrated, arm, cortexa9, processor, dedicated, accelerator, architecture, configured, realize, various, objects, detection, simply, loading, different, parameters, 2d, parallelism, involved, accelerator, unit, combination, brings, flexibility, scheme, implemented, xilinx, zc702, platform, experiment, result, shows, 40, qvga, frames, per, second, achieved, realtime, face, detection, accelerator, achieves, 13, times, improvement, opencv, implementation, standalone, cortexa9, wrt, execution, speed, meanwhile, accelerator, consumes, 40, less, fpga, hardware, resources, priorart, implementation]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', \n",
    "                     'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', \n",
    "                     'CZI', 'www']\n",
    "\n",
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))\n",
    "\n",
    "# Tokenize the text\n",
    "df_tokenized = df_cleaned.withColumn(\"tokenized_abstract\", split(col(\"cleaned_abstract\"), \" \"))\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokenized_abstract\", outputCol=\"filtered_abstract\", \n",
    "                           stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)\n",
    "\n",
    "# Show the cleaned dataframe\n",
    "df_filtered.select(\"abstract\", \"cleaned_abstract\", \"tokenized_abstract\", \"filtered_abstract\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b56fc2b-d63b-4e8f-bb12-76ad3e5c7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|               title|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|4aa69add-3978-480...|Dynamic analysis ...|(20000,[28,42,274...|\n",
      "|4ab3735c-80f1-472...|A new approach of...|(20000,[78,274,46...|\n",
      "|00127ee2-cb05-48c...|Preliminary Desig...|(20000,[1072,1241...|\n",
      "|001eef4f-1d00-4ae...|A Heterogeneous S...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Apply TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_abstract\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "df_featurized = hashingTF.transform(df_filtered)\n",
    "\n",
    "# Apply IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_featurized)\n",
    "df_vectorized = idf_model.transform(df_featurized)\n",
    "\n",
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "df_vectorized.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5cd8765-4ec7-4aec-b347-71e5dcd77ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langid\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import StringType\n",
    "# from pyspark.ml.feature import HashingTF, IDF, PCA\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "# import langid\n",
    "\n",
    "# # Initialize Spark session with maxResultSize configuration\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"PCA_KMeans\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Load the dataset\n",
    "# df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)\n",
    "\n",
    "# # Apply TF-IDF to the filtered abstract\n",
    "# hashingTF = HashingTF(inputCol=\"filtered_abstract\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "# df_featurized = hashingTF.transform(df_filtered)\n",
    "\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idf_model = idf.fit(df_featurized)\n",
    "# df_vectorized = idf_model.transform(df_featurized)\n",
    "\n",
    "# # Select only the columns we need\n",
    "# df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "# df_vectorized.show(5)\n",
    "\n",
    "# # Sample the dataframe for PCA and KMeans due to large size\n",
    "# def sample_data(df, fraction, max_attempts=5):\n",
    "#     attempt = 0\n",
    "#     sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "#     while sampled_df.isEmpty() and attempt < max_attempts:\n",
    "#         fraction *= 2  # Increase fraction to get more data\n",
    "#         sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "#         attempt += 1\n",
    "#     if sampled_df.isEmpty():\n",
    "#         raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "#     return sampled_df\n",
    "\n",
    "# # Sample the data with initial fraction 0.1\n",
    "# df_sampled = sample_data(df_vectorized, 0.1)\n",
    "\n",
    "# # Verify that the sampled DataFrame is not too large\n",
    "# print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "\n",
    "# # Apply PCA to reduce dimensions (e.g., to 20 components)\n",
    "# pca = PCA(k=20, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "# pca_model = pca.fit(df_sampled)\n",
    "# df_pca = pca_model.transform(df_sampled)\n",
    "\n",
    "# # Determine the optimal number of clusters using the elbow method\n",
    "# costs = []\n",
    "# for k in range(2, 21):\n",
    "#     kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "#     model = kmeans.fit(df_pca)\n",
    "#     cost = model.summary.trainingCost\n",
    "#     costs.append(cost)\n",
    "\n",
    "# # Plot the elbow curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(2, 21), costs, marker='o')\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Cost')\n",
    "# plt.title('Elbow Method For Optimal k')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a4bccb3-f616-47c2-82e1-d3d72523bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|               title|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|4aa69add-3978-480...|Dynamic analysis ...|(20000,[28,42,274...|\n",
      "|4ab3735c-80f1-472...|A new approach of...|(20000,[78,274,46...|\n",
      "|00127ee2-cb05-48c...|Preliminary Desig...|(20000,[1072,1241...|\n",
      "|001eef4f-1d00-4ae...|A Heterogeneous S...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Sampled DataFrame count: 3\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|               title|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|4aa69add-3978-480...|Dynamic analysis ...|(20000,[28,42,274...|\n",
      "|00127ee2-cb05-48c...|Preliminary Desig...|(20000,[1072,1241...|\n",
      "|001eef4f-1d00-4ae...|A Heterogeneous S...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Cannot aggregate object of size 1600080000 Bytes, as it's bigger than maxResultSize (1073741824 Bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Apply PCA to reduce dimensions (e.g., to 20 components)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_sampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m df_pca \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(df_sampled)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Determine the optimal number of clusters using the elbow method\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Cannot aggregate object of size 1600080000 Bytes, as it's bigger than maxResultSize (1073741824 Bytes)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark session with maxResultSize configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PCA_KMeans\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)\n",
    "\n",
    "# Apply TF-IDF to the filtered abstract\n",
    "hashingTF = HashingTF(inputCol=\"filtered_abstract\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "df_featurized = hashingTF.transform(df_filtered)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_featurized)\n",
    "df_vectorized = idf_model.transform(df_featurized)\n",
    "\n",
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "df_vectorized.show(5)\n",
    "\n",
    "# Sample the dataframe for PCA and KMeans due to large size\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.count() == 0 and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.count() == 0:\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "# Sample the data with initial fraction 0.1\n",
    "df_sampled = sample_data(df_vectorized, 0.1)\n",
    "\n",
    "# Verify that the sampled DataFrame is not too large\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "df_sampled.show(5)\n",
    "\n",
    "# Apply PCA to reduce dimensions (e.g., to 20 components)\n",
    "pca = PCA(k=20, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_sampled)\n",
    "df_pca = pca_model.transform(df_sampled)\n",
    "\n",
    "# Determine the optimal number of clusters using the elbow method\n",
    "costs = []\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = kmeans.fit(df_pca)\n",
    "    cost = model.summary.trainingCost\n",
    "    costs.append(cost)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 21), costs, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8621ccea-6ba6-4c61-9ccc-275ab0e62979",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the K-means model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(k\u001b[38;5;241m=\u001b[39moptimal_k, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdf_pca\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df_clusters \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df_pca)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_pca' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume the optimal number of clusters is determined to be 10\n",
    "optimal_k = 10\n",
    "\n",
    "# Train the K-means model\n",
    "kmeans = KMeans(k=optimal_k, seed=1, featuresCol=\"pca_features\")\n",
    "model = kmeans.fit(df_pca)\n",
    "\n",
    "# Make predictions\n",
    "df_clusters = model.transform(df_pca)\n",
    "df_clusters.select(\"id\", \"title\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901961d4-9db8-4b4f-8cdc-d30e3491a66b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recommendations\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m recommended_papers \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA new approach of....\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m recommended_papers:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mrecommend_papers\u001b[0;34m(title, top_n)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommend_papers\u001b[39m(title, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Find the cluster of the given paper title\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     paper_cluster \u001b[38;5;241m=\u001b[39m \u001b[43mdf_clusters\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(df_clusters\u001b[38;5;241m.\u001b[39mtitle \u001b[38;5;241m==\u001b[39m title)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Get all papers in the same cluster\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     cluster_papers \u001b[38;5;241m=\u001b[39m df_clusters\u001b[38;5;241m.\u001b[39mfilter(df_clusters\u001b[38;5;241m.\u001b[39mprediction \u001b[38;5;241m==\u001b[39m paper_cluster)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return float(dot_product / (norm_v1 * norm_v2))\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Build search engine function\n",
    "def recommend_papers(title, top_n=5):\n",
    "    # Find the cluster of the given paper title\n",
    "    paper_cluster = df_clusters.filter(df_clusters.title == title).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    # Get all papers in the same cluster\n",
    "    cluster_papers = df_clusters.filter(df_clusters.prediction == paper_cluster)\n",
    "    \n",
    "    # Get the features of the given paper\n",
    "    paper_features = df_clusters.filter(df_clusters.title == title).select(\"features\").collect()[0][0]\n",
    "    \n",
    "    # Calculate cosine similarity and recommend top N papers\n",
    "    cluster_papers = cluster_papers.withColumn(\"similarity\", cosine_similarity_udf(cluster_papers.features, Vectors.dense(paper_features.toArray())))\n",
    "    recommendations = cluster_papers.orderBy(\"similarity\", ascending=False).limit(top_n)\n",
    "    \n",
    "    return recommendations.select(\"title\", \"similarity\").collect()\n",
    "\n",
    "# Example usage\n",
    "recommended_papers = recommend_papers(\"A new approach of....\", top_n=5)\n",
    "for rec in recommended_papers:\n",
    "    print(f\"Title: {rec['title']}, Similarity: {rec['similarity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850ce04-3b41-42a0-a55d-b899910a84d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74704fec-13d2-48cd-bb17-6916b62d65b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6001ae1-3cdd-4bc1-a203-5c46f60ac5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4283b1-ec99-469b-8f4a-60733b7939a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile('dblp.v10.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('dblp_data')\n",
    "\n",
    "# List the contents of the directory to verify\n",
    "print(os.listdir('dblp_data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fac1c-7ec6-43dd-90b2-f37579b99301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Directory containing the original dataset\n",
    "original_dir = 'dblp_data'\n",
    "# Directory to store copies\n",
    "copies_dir = 'dblp_data_copies'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(copies_dir, exist_ok=True)\n",
    "\n",
    "# Create 1000 copies\n",
    "for i in range(1000):\n",
    "    dest_dir = os.path.join(copies_dir, f'dblp_copy_{i}')\n",
    "    shutil.copytree(original_dir, dest_dir)\n",
    "    \n",
    "print(f\"Created {len(os.listdir(copies_dir))} copies of the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6502621-bef3-40ce-9adf-b9b6b247e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import langid\n",
    "\n",
    "# Initialize Spark session with the desired configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PCA_KMeans\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.json(\"dblp_data/*.json\", multiLine=True)\n",
    "\n",
    "# Sample the dataframe for PCA and KMeans due to large size\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.isEmpty() and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.isEmpty():\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "# Sample the data with initial fraction 0.1\n",
    "df_sampled = sample_data(df, 0.1)\n",
    "\n",
    "# Verify that the sampled DataFrame is not too large\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "\n",
    "# Apply PCA to reduce dimensions (e.g., to 20 components)\n",
    "pca = PCA(k=20, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_sampled)\n",
    "df_pca = pca_model.transform(df_sampled)\n",
    "\n",
    "# Determine the optimal number of clusters using the elbow method\n",
    "costs = []\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = kmeans.fit(df_pca)\n",
    "    cost = model.summary.trainingCost\n",
    "    costs.append(cost)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 21), costs, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb7da8-2857-45f6-a8bc-982d4d4064ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601261c-1198-41e2-8234-11e3018b9e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba95316-2696-4358-a1ae-ccad72022413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe22eb-07cc-4aa3-b620-62cae402983c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
