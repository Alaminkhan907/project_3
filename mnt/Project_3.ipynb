{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf04a4-3530-4460-86a7-10f7a29eb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext represents the connection to a Spark cluster\n",
    "from pyspark.context import SparkContext\n",
    "# Configuration for a Spark application\n",
    "from pyspark.conf import SparkConf\n",
    "# The entry point to programming Spark with the Dataset and DataFrame API\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "conf = SparkConf().setAppName(\"Project_session_3_SparkML\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True) # OK for exploration, not great for performance\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456ca4c-9061-4fa6-9143-7f2e85f553c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -u dblp.v10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b4cd2-6bc5-4abb-aaea-3178e863e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc5e62-27f0-4fbc-8177-b9e3eb81967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()   # Examine the structure of the data\n",
    "df.show(5)         # Display a few sample rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1250b6-f3d2-4a1d-a7ec-7248d574e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, LongType\n",
    "\n",
    "# Correctly define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"abstract\", StringType(), True),\n",
    "    StructField(\"authors\", ArrayType(StringType()), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"n_citation\", LongType(), True),\n",
    "    StructField(\"references\", ArrayType(StringType()), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"venue\", StringType(), True),\n",
    "    StructField(\"year\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Read the data with the schema\n",
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3e8c3-6c28-4e45-ae5d-4a211791f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()  # Display the schema to verify it's correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a42ffe-f894-4892-b67b-0123b215bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b6add-241b-40a4-8303-0b5312e4951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# from langdetect import detect\n",
    "\n",
    "# Filter for English documents\n",
    "df_english = df.filter(col(\"title\") == \"en\") \n",
    "\n",
    "# Basic data exploration\n",
    "df_english.describe([\"year\", \"n_citation\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688cd02-5953-40c5-8a62-9ba92eeae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46e8d5-1cd4-4d8b-84b3-80bf7cf0ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark dataframes can be interoperable with pandas too\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82d3a1-9f9e-48cb-8fe4-7204bc402140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to detect language (you might want to optimize this for Spark)\n",
    "# def detect_language(text):\n",
    "#     try:\n",
    "#         return detect(text)\n",
    "#     except:\n",
    "#         return \"unknown\"\n",
    "\n",
    "# # Register the function as a Spark UDF\n",
    "# from pyspark.sql.functions import udf\n",
    "# detect_language_udf = udf(detect_language, StringType())\n",
    "\n",
    "# # Filter for English documents based on either title or abstract\n",
    "# df_english = df.filter(\n",
    "#     (detect_language_udf(col(\"title\")) == \"en\") | (detect_language_udf(col(\"abstract\")) == \"en\")\n",
    "# )\n",
    "\n",
    "# # Optionally, drop the now-unnecessary \"value\" column\n",
    "# #df_english = df_english.drop(\"value\")\n",
    "\n",
    "# # Display some sample English documents\n",
    "# df_english.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002b946-9b5d-4038-af18-26fa3fe037b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"title\").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3674e9-176f-4e2d-b2b5-f4c559be65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a0a75-567f-4146-80f3-96bd93c9f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# 1. Tokenization (Split into Words)\n",
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df_english)\n",
    "\n",
    "# 2. Remove Stop Words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filteredData = stopwordsRemover.transform(wordsData)\n",
    "\n",
    "# 3. Remove Custom Stop Words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "stopwordsRemover_custom = StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered_custom\", stopWords=custom_stop_words)\n",
    "filteredData_custom = stopwordsRemover_custom.transform(filteredData)\n",
    "\n",
    "# 4. Remove Punctuation (Using Regex)\n",
    "removePunctuation = regexp_replace(concat_ws(\" \", col(\"filtered_custom\")), r'[!()-[]{};:\\'\"\\\\,<>./?@#$%^&*_~]', \" \")\n",
    "filteredData_custom = filteredData_custom.withColumn(\"filtered_no_punc\", removePunctuation)\n",
    "\n",
    "# 5. Lowercase Conversion\n",
    "lowercase = lower(col(\"filtered_no_punc\"))\n",
    "filteredData_custom = filteredData_custom.withColumn(\"filtered_lower\", lowercase)\n",
    "\n",
    "# 6. Select relevant columns for further processing.\n",
    "preprocessed_df = filteredData_custom.select(\"id\", \"title\", \"filtered_lower\")\n",
    "\n",
    "# Display the first 5 preprocessed abstracts\n",
    "preprocessed_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a1ed9-a570-4c0d-a2f3-950ceee1ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# 1. Tokenization (If not done in the preprocessing stage)\n",
    "tokenizer = Tokenizer(inputCol=\"filtered_lower\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(preprocessed_df)\n",
    "\n",
    "# 2. Calculate Term Frequencies (TF)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)  # Choose a suitable numFeatures\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "# 3. Calculate Inverse Document Frequencies (IDF)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47324205-0c3e-42ed-ba24-247768075d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3da249-b6ff-4a69-8b74-14825ac70a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace0cd8-92d3-4ce7-9f17-4f989b4d704d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1e0eb-9431-43a6-bd53-d11abf52f9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636a84d-002b-462b-a512-fb1de4d0bf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783b637-ef75-4872-a58e-1b731b977f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6b56f-0a70-42bf-a094-8042f704852a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d391271-a9e4-4826-b06a-3bb1d4226e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
