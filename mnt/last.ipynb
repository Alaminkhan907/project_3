{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9699974-be2a-4bdb-bab1-fefe88ed080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in /opt/conda/lib/python3.11/site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from langid) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "!pip install langid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import StringType, IntegerType \n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "import langid\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.sql.functions import col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29fc7d59-1a8f-4cd0-9ac1-15c7880d8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Project 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data\n",
    "# df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)\n",
    "df=spark.read.json(\"./dblp-ref/dblp-ref-0.json\")\n",
    "# df = spark.read.json([\"./dblp-ref/dblp-ref-0.json\", \"./dblp-ref/dblp-ref-1.json\", \"./dblp-ref/dblp-ref-2.json\", \"./dblp-ref/dblp-ref-3.json\"], multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "209b4115-5208-42cc-a0dd-93faab263914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "Total number of records: 1000000\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "|                NULL|[Rafael Álvarez, ...|005ce28f-ed77-4e9...|         0|                NULL|COMPARING GNG3D A...|international con...|2009|\n",
      "|                NULL|[Jovan Dj. Golic,...|00638a94-23bf-4fa...|         2|                NULL|Vectorial fast co...|                    |2004|\n",
      "|                NULL|[Guzin Ulutas, Mu...|00701b05-684f-45f...|         0|[5626736c-e434-4e...|Improved Secret I...|international sym...|2011|\n",
      "|                NULL|[Pranay Chaudhuri...|00745041-3636-4d1...|         0|                NULL|A Self-Stabilizin...|parallel and dist...|2003|\n",
      "|                NULL|[Dominik Szajerma...|00964544-cbe2-4da...|         0|[3fcd7cdc-20e6-4e...|Fur Visualisation...|international con...|2014|\n",
      "|Recent achievemen...|[Ankita Brahmacha...|00a119c4-d367-460...|         0|[84d47128-58d0-41...|Identifying Psych...|                    |2013|\n",
      "|                NULL|[David Al-Dabass,...|00ba1eb4-d1aa-458...|         0|                NULL|Simulation of a v...|                    |1995|\n",
      "|Recently, Bridges...|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|        50|                  []|Multisymplectic S...|international con...|2002|\n",
      "|Most applications...|[Patrick Cousot, ...|00c85316-bddf-4bc...|        50|[6e8a3ec3-9a99-4f...|Relational Abstra...|                    |1991|\n",
      "|Three speech trai...|[Minoru Shigenaga...|00ca027b-5174-40f...|         0|                NULL|Speech training s...|international joi...|1979|\n",
      "|                NULL|[Reiko Heckel, To...|00dc2bba-3237-4d4...|        50|                NULL|Software Evolutio...|Electronic Notes ...|2003|\n",
      "|This paper focuse...|[Efthymios Alepis...|00dd5ece-1339-4cb...|        50|[522bea13-bd45-49...|Knowledge Enginee...|joint conference ...|2008|\n",
      "|                NULL|[Milos Zelezný, P...|00e02aeb-b424-4ca...|         7|                NULL|Design of an audi...|conference of the...|2002|\n",
      "|                NULL|[Abdullah Alsubai...|00e3940b-201b-40b...|         6|[0293844d-7244-45...|A Platform for Di...|                    |2013|\n",
      "|                NULL|[Tien Ho-Phuoc, A...|00fd4f6f-c7a1-49a...|         3|                NULL|A COMPUTATIONAL S...|international con...|2009|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema\n",
    "df.printSchema()\n",
    "total_records = df.count()\n",
    "print(\"Total number of records:\", total_records)\n",
    "\n",
    "df.show(20)\n",
    "# print(f\"Number of records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6fbc1283-4a43-4f04-9a5c-78d8046f3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+----------------+\n",
      "|summary|            abstract|                  id|        n_citation|                                title|               venue|            year|\n",
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+----------------+\n",
      "|  count|              753646|             1000000|           1000000|                              1000000|             1000000|         1000000|\n",
      "|   mean|                NULL|                NULL|          30.87573|                                 NULL|                NULL|     2006.365124|\n",
      "| stddev|                NULL|                NULL|137.94247310778852|                                 NULL|                NULL|7.83383289543257|\n",
      "|    min|\u000e\u000f\u0010\u0011 \u000e\u000f\u0010\u0011\u0012\u0013\u0014\u0015\u0015\u0016\u0017\u0018...|000000b8-7f59-49a...|                 0|                 ! and ? – Storage...|                    |            1937|\n",
      "|    max|�����������������...|ffffe1e6-981e-4cf...|             73362|インパルス応答測定のための有色疑似...|worst-case execut...|            2017|\n",
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+----------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|  246354|      0|  0|         0|    138520|    0|    0|   0|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|        n_citation|\n",
      "+-------+------------------+\n",
      "|  count|           1000000|\n",
      "|   mean|          30.87573|\n",
      "| stddev|137.94247310778852|\n",
      "|    min|                 0|\n",
      "|    max|             73362|\n",
      "+-------+------------------+\n",
      "\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|    NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|    NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "|    NULL|[Rafael Álvarez, ...|005ce28f-ed77-4e9...|         0|                NULL|COMPARING GNG3D A...|international con...|2009|\n",
      "|    NULL|[Jovan Dj. Golic,...|00638a94-23bf-4fa...|         2|                NULL|Vectorial fast co...|                    |2004|\n",
      "|    NULL|[Guzin Ulutas, Mu...|00701b05-684f-45f...|         0|[5626736c-e434-4e...|Improved Secret I...|international sym...|2011|\n",
      "|    NULL|[Pranay Chaudhuri...|00745041-3636-4d1...|         0|                NULL|A Self-Stabilizin...|parallel and dist...|2003|\n",
      "|    NULL|[Dominik Szajerma...|00964544-cbe2-4da...|         0|[3fcd7cdc-20e6-4e...|Fur Visualisation...|international con...|2014|\n",
      "|    NULL|[David Al-Dabass,...|00ba1eb4-d1aa-458...|         0|                NULL|Simulation of a v...|                    |1995|\n",
      "|    NULL|[Reiko Heckel, To...|00dc2bba-3237-4d4...|        50|                NULL|Software Evolutio...|Electronic Notes ...|2003|\n",
      "|    NULL|[Milos Zelezný, P...|00e02aeb-b424-4ca...|         7|                NULL|Design of an audi...|conference of the...|2002|\n",
      "|    NULL|[Abdullah Alsubai...|00e3940b-201b-40b...|         6|[0293844d-7244-45...|A Platform for Di...|                    |2013|\n",
      "|    NULL|[Tien Ho-Phuoc, A...|00fd4f6f-c7a1-49a...|         3|                NULL|A COMPUTATIONAL S...|international con...|2009|\n",
      "|    NULL|[Marco Baroni, Fr...|01082175-d4e3-456...|       101|                NULL|Cleaneval: a Comp...|language resource...|2008|\n",
      "|    NULL|[Nebojsa M. Ralev...|010fda73-e708-489...|         0|[17d92951-cd71-49...|Algorithms for th...|                    |2007|\n",
      "|    NULL|[William Perrizo,...|01103168-55d8-42b...|         0|                NULL|Vertical Database...|                    |2005|\n",
      "|    NULL|[Norihiko Moriwak...|014df191-abb2-44c...|         0|[08704b8d-70a1-44...|Sensor-Data-Drive...|                    |2014|\n",
      "|    NULL|[Prudence W. H. W...|01546472-0ac9-47f...|         6|[08784bb3-09b9-4b...|An 8/3 Lower Boun...|international sym...|2012|\n",
      "|    NULL|[Olivia Mendoza, ...|016eba4d-f912-492...|        50|[159fdfd2-921e-42...|Comparison of Fuz...|      soft computing|2010|\n",
      "|    NULL|[Martine Collard,...|01733dc2-4910-40d...|         3|[0102f234-e30b-4b...|Extracted Knowled...|research challeng...|2007|\n",
      "|    NULL|[Yosuke Furukawa,...|019a51de-5136-41c...|        50|                NULL|Context Dependent...|Journal of Advanc...|2007|\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Check for missing values (excluding `isnan`)\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Distribution of citations\n",
    "df.select(\"n_citation\").describe().show()\n",
    "\n",
    "# Check for null abstracts and titles\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe239df3-6de6-4f89-91fe-82cd2d992dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|language|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|      en|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|      en|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|      en|\n",
      "|Recent achievemen...|[Ankita Brahmacha...|00a119c4-d367-460...|         0|[84d47128-58d0-41...|Identifying Psych...|                    |2013|      en|\n",
      "|Recently, Bridges...|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|        50|                  []|Multisymplectic S...|international con...|2002|      en|\n",
      "|Most applications...|[Patrick Cousot, ...|00c85316-bddf-4bc...|        50|[6e8a3ec3-9a99-4f...|Relational Abstra...|                    |1991|      en|\n",
      "|Three speech trai...|[Minoru Shigenaga...|00ca027b-5174-40f...|         0|                NULL|Speech training s...|international joi...|1979|      en|\n",
      "|This paper focuse...|[Efthymios Alepis...|00dd5ece-1339-4cb...|        50|[522bea13-bd45-49...|Knowledge Enginee...|joint conference ...|2008|      en|\n",
      "|Embedded systems ...|[Matias Madou, Bj...|01047814-b615-444...|        50|                NULL|Link-time compact...|european symposiu...|2004|      en|\n",
      "|Xax is a browser ...|[John R. Douceur,...|010d4ce9-0279-416...|        50|[0abc9de7-e047-44...|Leveraging legacy...|operating systems...|2008|      en|\n",
      "|In recent years, ...|[Hiroshi Furukawa...|010d9907-45ef-459...|         7|[b2f0e0d3-0071-40...|A pedestrian navi...|international con...|2013|      en|\n",
      "|Previous language...|[Carmen Fernández...|012b88ae-a763-45d...|        50|[00515e82-3da6-49...|Word pairs in lan...|                    |2004|      en|\n",
      "|Spatial encryptio...|[Michel Abdalla, ...|016a9a21-e882-4cd...|        50|[3140c9ba-8d98-42...|Leakage-Resilient...|                    |2012|      en|\n",
      "|In system operati...|[Mark Burgess, Al...|01705f09-d395-4a0...|         8|[363278e2-6c9d-45...|On system rollbac...|The Journal of Lo...|2011|      en|\n",
      "|Business strategy...|[Constantinos Gia...|01b6f2ca-3903-419...|        12|[0ce446cf-4f4f-49...|Model-Driven Stra...|                    |2012|      en|\n",
      "|FTP Mirror Tracke...|[Alexei Novikov, ...|01ccb92f-46f1-400...|         0|                  []|FTP Mirror Tracke...|usenix large inst...|2000|      en|\n",
      "|There are a numbe...|[Theresa Beauboue...|01edeac9-cd8b-46f...|         0|[09b666de-6279-48...|Information Syste...|                    |2014|      en|\n",
      "|Breast cancer is ...|[Bartosz Krawczyk...|0265aea8-65f3-4f4...|        50|[16f0341b-538d-40...|Breast Cancer Ide...|                    |2013|      en|\n",
      "|The development o...|[J. Niblock, Jian...|028d37c8-b571-41b...|         0|[8fc06656-ed2a-46...|Automated Object ...|international con...|2008|      en|\n",
      "|Quality specified...|[Qiong Liu, You Y...|02a5e8a8-061e-4c1...|         3|[04f8d077-765a-41...|Quality Assessmen...|conference on mul...|2013|      en|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter only English documents\n",
    "# Function to detect language using langid\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Add a new column for language detection\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "\n",
    "# Filter only English documents\n",
    "df = df.filter(df.language == 'en')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0831673f-7879-49cd-80c3-9fd96763d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8909ec04-a86c-4067-8f78-a865aa52ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_abstract\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a42bc8c-d4f1-4f3f-8369-425021008da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0804d576-3882-475c-aed0-bf9f1a316a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o4184.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train a Word2Vec model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m Word2Vec(vectorSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, minCount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_filtered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_vectorized \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df_filtered)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o4184.fit"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"filtered_words\", outputCol=\"word2vec_features\")\n",
    "model = word2vec.fit(df_filtered)\n",
    "df_vectorized = model.transform(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86fc5262-4383-4412-953d-c0192f14a77c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select only the columns we need\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_vectorized \u001b[38;5;241m=\u001b[39m \u001b[43mdf_vectorized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword2vec_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2764\u001b[0m, in \u001b[0;36mDataFrame._jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2763\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2751\u001b[0m, in \u001b[0;36mDataFrame._jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jseq\u001b[39m(\n\u001b[1;32m   2746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2747\u001b[0m     cols: Sequence,\n\u001b[1;32m   2748\u001b[0m     converter: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m, JavaObject]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2749\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:88\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 88\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 88\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:63\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     61\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39m_jc\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:56\u001b[0m, in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_from_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     55\u001b[0m     sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241m.\u001b[39mcol(name)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"word2vec_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cf243-99dc-40f6-ae71-0f16ec697167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UDF for converting sparse to dense vectors\n",
    "def to_dense(v):\n",
    "    if isinstance(v, SparseVector):\n",
    "        return DenseVector(v.toArray())\n",
    "    elif isinstance(v, DenseVector):\n",
    "        return v\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported vector type\")\n",
    "\n",
    "to_dense_udf = udf(to_dense, VectorUDT())\n",
    "\n",
    "# Apply UDF to convert features to dense vectors\n",
    "df_dense = df_vectorized.withColumn('dense_features', to_dense_udf(col('word2vec_features')))\n",
    "\n",
    "# Select only the columns we need\n",
    "df_dense = df_dense.select(\"id\", \"title\", \"dense_features\")\n",
    "df_dense.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f544e-42bd-48b9-a6fe-cdbb57e76e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.count() == 0 and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.count() == 0:\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "df_sampled = sample_data(df_vectorized, 0.1)\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "df_sampled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcbe39-1cf6-4eff-9459-577f24dc2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use PCA to reduce dimensions\n",
    "# pca = PCA(k=2, inputCol=\"word2vec_features\", outputCol=\"pca_features\")\n",
    "# # df_sampled_repartitioned = df_sampled.repartition(100)\n",
    "# # df_sampled_repartitioned.persist()\n",
    "# pca_model = pca.fit(pca)\n",
    "# df_pca = pca_model.transform(df_sampled_repartitioned)\n",
    "# df_pca.show()\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"word2vec_features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_sampled)\n",
    "df_pca = pca_model.transform(df_sampled)\n",
    "df_pca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25e91b-c8b3-4df8-bf1a-7278e17f23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Elbow Method to Find Optimal K\n",
    "# costs = []\n",
    "# for k in range(2, 21):\n",
    "#     kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "#     model = kmeans.fit(df_pca)\n",
    "#     costs.append(model.summary.trainingCost)\n",
    "\n",
    "# plt.plot(range(2, 21), costs, marker='o')\n",
    "# plt.xlabel('Number of Clusters (k)')\n",
    "# plt.ylabel('Cost')\n",
    "# plt.title('Elbow Method for Optimal k')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b8911-c143-4761-a6d8-a7faf7df36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KMeans Clustering (using the optimal k from the elbow method)\n",
    "# optimal_k = 10  # Set this to the optimal value from the elbow method\n",
    "# kmeans = KMeans(k=optimal_k, seed=1, featuresCol=\"pca_features\")\n",
    "# model = kmeans.fit(df_pca)\n",
    "# df_clustered = model.transform(df_pca)\n",
    "# df_clustered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448eae65-cc53-49e6-a119-c797ada84121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Apply Bisecting K-means clustering\n",
    "bisecting_kmeans = BisectingKMeans(k=100, seed=1, featuresCol=\"pca_features\")\n",
    "model = bisecting_kmeans.fit(df_pca)\n",
    "df_clustered = model.transform(df_pca)\n",
    "df_clustered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044dcfb-a28e-4a23-b64f-8a75c640a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# # Apply Gaussian Mixture Model clustering\n",
    "# gmm = GaussianMixture(k=20, seed=1, featuresCol=\"pca_features\")\n",
    "# model = gmm.fit(df_pca)\n",
    "# df_clustered = model.transform(df_pca)\n",
    "# df_clustered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028e3cc-6722-4ef1-a5c3-4b79a693e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# # Apply K-means clustering with 20 clusters\n",
    "# kmeans = KMeans(k=100, seed=1, featuresCol=\"pca_features\")\n",
    "# model = kmeans.fit(df_pca)\n",
    "# df_clustered = model.transform(df_pca)\n",
    "# df_clustered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac7419-70a1-4140-bb22-cee2914e6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Elbow Method to Find Optimal K for Bisecting K-means\n",
    "costs = []\n",
    "k_values = range(2, 41)  # Trying K values from 2 to 100\n",
    "\n",
    "for k in k_values:\n",
    "    bisecting_kmeans = BisectingKMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = bisecting_kmeans.fit(df_pca)\n",
    "    costs.append(model.computeCost(df_pca))\n",
    "\n",
    "# Plot the costs against K values\n",
    "plt.plot(k_values, costs, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method for Optimal k using Bisecting K-means')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4367b-90e7-48c3-a2b8-09f5021a26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Elbow Method to Find Optimal K\n",
    "# costs = []\n",
    "# for k in range(2, 41):\n",
    "#     kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "#     model = kmeans.fit(df_pca)\n",
    "#     costs.append(model.summary.trainingCost)\n",
    "\n",
    "# plt.plot(range(2, 41), costs, marker='o')\n",
    "# plt.xlabel('Number of Clusters (k)')\n",
    "# plt.ylabel('Cost')\n",
    "# plt.title('Elbow Method for Optimal k')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b259a1-b086-4aab-accd-d1b1635fd360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a6460-97f9-4a62-a6a1-ea5435e372ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic search engine function\n",
    "def recommend_papers(title, top_n=5):\n",
    "    paper_cluster = df_clustered.filter(df_clustered.title == title).select(\"prediction\").collect()[0][0]\n",
    "    cluster_papers = df_clustered.filter(df_clustered.prediction == paper_cluster)\n",
    "    paper_features = np.array(cluster_papers.filter(cluster_papers.title == title).select(\"pca_features\").collect()[0][0])\n",
    "    similarities = []\n",
    "\n",
    "    for row in cluster_papers.collect():\n",
    "        other_title = row[\"title\"]\n",
    "        other_features = np.array(row[\"pca_features\"])\n",
    "        similarity = cosine_similarity([paper_features], [other_features])[0][0]\n",
    "        similarities.append((other_title, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "recommended_papers = recommend_papers(\"example_paper_title\", top_n=2)\n",
    "print(recommended_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa64d91-2ab3-46b0-8ad2-c8afcd10d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a6281-48fe-4c4a-a472-cc14e5f4179c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05be32-c25c-496c-b7a9-0522b1122da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab598e6-756b-4b83-8c06-e7029d907768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88ff84-6146-42e1-b207-8172143d92a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb0c2a-1e22-4ae3-9954-ef456f798c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd116d-3776-441d-af38-86bf0525e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Project 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data\n",
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)\n",
    "\n",
    "# Show the schema\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Number of records: {df.count()}\")\n",
    "\n",
    "# Check for missing values\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Filter only English documents\n",
    "# df = df.filter(df.language == 'en')\n",
    "\n",
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_abstract\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)\n",
    "\n",
    "# Remove stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"filtered_words\", outputCol=\"word2vec_features\")\n",
    "model = word2vec.fit(df_filtered)\n",
    "df_vectorized = model.transform(df_filtered)\n",
    "\n",
    "# Define UDF for converting sparse to dense vectors\n",
    "def to_dense(v):\n",
    "    if isinstance(v, SparseVector):\n",
    "        return DenseVector(v.toArray())\n",
    "    elif isinstance(v, DenseVector):\n",
    "        return v\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported vector type\")\n",
    "\n",
    "to_dense_udf = udf(to_dense, VectorUDT())\n",
    "\n",
    "# Apply UDF to convert features to dense vectors\n",
    "df_dense = df_vectorized.withColumn('dense_features', to_dense_udf(col('word2vec_features')))\n",
    "\n",
    "# Select only the columns we need\n",
    "df_dense = df_dense.select(\"id\", \"title\", \"dense_features\")\n",
    "\n",
    "# Sample the data\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.count() == 0 and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.count() == 0:\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "df_sampled = sample_data(df_dense, 0.1)\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "df_sampled.show(5)\n",
    "\n",
    "# Use PCA to reduce dimensions\n",
    "pca = PCA(k=2, inputCol=\"dense_features\", outputCol=\"pca_features\")\n",
    "df_sampled_repartitioned = df_sampled.repartition(100)\n",
    "df_sampled_repartitioned.persist()\n",
    "pca_model = pca.fit(df_sampled_repartitioned)\n",
    "df_pca = pca_model.transform(df_sampled_repartitioned)\n",
    "df_pca.show()\n",
    "\n",
    "# Elbow Method to Find Optimal K\n",
    "costs = []\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = kmeans.fit(df_pca)\n",
    "    costs.append(model.summary.trainingCost)\n",
    "\n",
    "plt.plot(range(2, 21), costs, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# KMeans Clustering (using the optimal k from the elbow method)\n",
    "optimal_k = 10  # Set this to the optimal value from the elbow method\n",
    "kmeans = KMeans(k=optimal_k, seed=1, featuresCol=\"pca_features\")\n",
    "model = kmeans.fit(df_pca)\n",
    "df_clustered = model.transform(df_pca)\n",
    "\n",
    "# Basic search engine function\n",
    "def recommend_papers(title, top_n=5):\n",
    "    paper_cluster = df_clustered.filter(df_clustered.title == title).select(\"prediction\").collect()[0][0]\n",
    "    cluster_papers = df_clustered.filter(df_clustered.prediction == paper_cluster)\n",
    "    paper_features = np.array(cluster_papers.filter(cluster_papers.title == title).select(\"pca_features\").collect()[0][0])\n",
    "    similarities = []\n",
    "\n",
    "    for row in cluster_papers.collect():\n",
    "        other_title = row[\"title\"]\n",
    "        other_features = np.array(row[\"pca_features\"])\n",
    "        similarity = cosine_similarity([paper_features], [other_features])[0][0]\n",
    "        similarities.append((other_title, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "recommended_papers = recommend_papers(\"example_paper_title\", top_n=5)\n",
    "print(recommended_papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fc98d-1cff-49d3-bc6a-8fb0c3ededac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
