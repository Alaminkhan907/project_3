{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6022a83f-a98a-49d7-b9b5-6be56e9a74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|Based on biologic...|[Guoping Pang, La...|4aa69add-3978-480...|         8|[04754a28-6bf4-4d...|Dynamic analysis ...|Mathematics and C...|2008|\n",
      "|In this paper, a ...|[S. Ben Jabra, Ez...|4ab3735c-80f1-472...|        50|[09cb2d7d-47d1-4a...|A new approach of...|international sym...|2008|\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|AdaBoost algorith...|[Zheng Xu, Runbin...|001eef4f-1d00-4ae...|         0|[0a11984c-ab6e-4b...|A Heterogeneous S...|high performance ...|2016|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "\n",
      "Number of records: 4\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|summary|            abstract|                  id|       n_citation|               title|               venue|              year|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|  count|                   4|                   4|                4|                   4|                   4|                 4|\n",
      "|   mean|                NULL|                NULL|             14.5|                NULL|                NULL|           2011.25|\n",
      "| stddev|                NULL|                NULL|23.96525262402492|                NULL|                NULL|3.9475730941089733|\n",
      "|    min|AdaBoost algorith...|00127ee2-cb05-48c...|                0|A Heterogeneous S...|Mathematics and C...|              2008|\n",
      "|    max|The purpose of th...|4ab3735c-80f1-472...|               50|Preliminary Desig...|international sym...|              2016|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|       0|      0|  0|         0|         0|    0|    0|   0|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|       n_citation|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|             14.5|\n",
      "| stddev|23.96525262402492|\n",
      "|    min|                0|\n",
      "|    max|               50|\n",
      "+-------+-----------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BD_Analytics_Clustering_Project\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)\n",
    "\n",
    "# Show the schema to understand the structure\n",
    "df.printSchema()\n",
    "\n",
    "# Display a sample of the data\n",
    "df.show(5)\n",
    "\n",
    "# Count the number of records\n",
    "print(f\"Number of records: {df.count()}\")\n",
    "\n",
    "# Display summary statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Check for missing values (excluding `isnan`)\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Distribution of citations\n",
    "df.select(\"n_citation\").describe().show()\n",
    "\n",
    "# Check for null abstracts and titles\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63314b8a-e761-4bac-bb42-a64483ed12e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langid\n",
      "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m885.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from langid) (1.26.4)\n",
      "Building wheels for collected packages: langid\n",
      "  Building wheel for langid (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=98be2d27530f3617580d67022e97a0c311c9ea18d14e781467fe0672f6fe89bf\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
      "Successfully built langid\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b4a4b3-556b-4a13-ab35-261452a9c655",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cld3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcld3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m udf\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringType\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cld3'"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to detect language using langid\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Add a new column for language detection\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "\n",
    "# Filter only English documents\n",
    "df = df.filter(df.language == 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1263a8f-5e28-4d88-b950-75bdac5948fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', \n",
    "                     'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', \n",
    "                     'CZI', 'www']\n",
    "\n",
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()-[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"cleaned_abstract\", outputCol=\"filtered_abstract\", \n",
    "                           stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_cleaned = remover.transform(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b69ecc-3166-43a8-832f-9e458fee8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"filtered_abstract\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)\n",
    "\n",
    "# Apply TF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "df_featurized = hashingTF.transform(df_tokenized)\n",
    "\n",
    "# Apply IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_featurized)\n",
    "df_vectorized = idf_model.transform(df_featurized)\n",
    "\n",
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "df_vectorized.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef407b-5705-460d-8c9e-5b8d229445ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensions while retaining 95% of the variance\n",
    "pca = PCA(k=100, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_vectorized)\n",
    "df_pca = pca_model.transform(df_vectorized)\n",
    "\n",
    "# Determine the optimal number of clusters using the elbow method\n",
    "costs = []\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = kmeans.fit(df_pca)\n",
    "    cost = model.summary.trainingCost\n",
    "    costs.append(cost)\n",
    "\n",
    "# Plot the elbow curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 21), costs, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cf472-8e74-4720-b7b5-8de8469a4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the optimal number of clusters is determined to be 10\n",
    "optimal_k = 10\n",
    "\n",
    "# Train the K-means model\n",
    "kmeans = KMeans(k=optimal_k, seed=1, featuresCol=\"pca_features\")\n",
    "model = kmeans.fit(df_pca)\n",
    "\n",
    "# Make predictions\n",
    "df_clusters = model.transform(df_pca)\n",
    "df_clusters.select(\"id\", \"title\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b48b7d-7a1f-4606-82e4-d9197f6ce766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Build search engine function\n",
    "def recommend_papers(title, top_n=5):\n",
    "    # Find the cluster of the given paper title\n",
    "    paper_cluster = df_clusters.filter(df_clusters.title == title).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    # Get all papers in the same cluster\n",
    "    cluster_papers = df_clusters.filter(df_clusters.prediction == paper_cluster)\n",
    "    \n",
    "    # Get the features of the given paper\n",
    "    paper_features = df_clusters.filter(df_clusters.title == title).select(\"features\").collect()[0][0]\n",
    "    \n",
    "    # Calculate cosine similarity and recommend top N papers\n",
    "    cluster_papers = cluster_papers.withColumn(\"similarity\", cosine_similarity_udf(cluster_papers.features, Vectors.dense(paper_features.toArray())))\n",
    "    recommendations = cluster_papers.orderBy(\"similarity\", ascending=False).limit(top_n)\n",
    "    \n",
    "    return recommendations.select(\"title\", \"similarity\").collect()\n",
    "\n",
    "# Example usage\n",
    "recommended_papers = recommend_papers(\"A new approach of....\", top_n=5)\n",
    "for rec in recommended_papers:\n",
    "    print(f\"Title: {rec['title']}, Similarity: {rec['similarity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99023126-5311-4ece-979d-e092dea10bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d7afc-6ba6-4893-b80e-93a33f9b35e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e6709-c183-4dc9-b516-dc1806b21fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ad630-d870-46bb-ad4d-aadbfd92543c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df1318-48df-4ea4-8626-cf22d7813da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02b036-17ed-44e9-b92a-ce767caabab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
