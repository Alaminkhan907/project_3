{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9699974-be2a-4bdb-bab1-fefe88ed080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langid\n",
      "  Using cached langid-1.1.6-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from langid) (1.26.4)\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "!pip install langid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import StringType, IntegerType \n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "import langid\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector, VectorUDT\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d0d013-36f6-4213-81db-f0a96e03dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100000s\") \\\n",
    "    .config(\"spark.network.timeout\", \"10000000s\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Project 3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3c5e1-341d-4cad-b33f-251531f33c94",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821c0608-d0a6-4576-8745-c97f23b07c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -u dblp.v10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28074814-2f67-469a-9282-c6adfd4d647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "# file_paths = [\"./dblp-ref/dblp-ref-0.json\", \"./dblp-ref/dblp-ref-1.json\", \"./dblp-ref/dblp-ref-2.json\", \"./dblp-ref/dblp-ref-3.json\"]\n",
    "# dfs = [spark.read.json(file_path) for file_path in file_paths]\n",
    "# df = reduce(lambda df1, df2: df1.union(df2), dfs)\n",
    "# df.show(5)\n",
    "\n",
    "# test with single dataset\n",
    "df = spark.read.json(\"./dblp-ref/dblp-ref-0.json\")\n",
    "\n",
    "df = df.limit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46151c-89fd-407e-afdb-e3df67c52a51",
   "metadata": {},
   "source": [
    "## Understanding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209b4115-5208-42cc-a0dd-93faab263914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of records: 1000\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "total_records = df.count()\n",
    "# print(\"Total number of records:\", total_records)\n",
    "\n",
    "df.show(5)\n",
    "print(f\"Number of records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbc1283-4a43-4f04-9a5c-78d8046f3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|     637|      0|  0|         0|       343|    0|    0|   0|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|       n_citation|\n",
      "+-------+-----------------+\n",
      "|  count|             1000|\n",
      "|   mean|           20.321|\n",
      "| stddev|85.92444919794346|\n",
      "|    min|                0|\n",
      "|    max|             2531|\n",
      "+-------+-----------------+\n",
      "\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|    NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|    NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "|    NULL|[Rafael Álvarez, ...|005ce28f-ed77-4e9...|         0|                NULL|COMPARING GNG3D A...|international con...|2009|\n",
      "|    NULL|[Jovan Dj. Golic,...|00638a94-23bf-4fa...|         2|                NULL|Vectorial fast co...|                    |2004|\n",
      "|    NULL|[Guzin Ulutas, Mu...|00701b05-684f-45f...|         0|[5626736c-e434-4e...|Improved Secret I...|international sym...|2011|\n",
      "|    NULL|[Pranay Chaudhuri...|00745041-3636-4d1...|         0|                NULL|A Self-Stabilizin...|parallel and dist...|2003|\n",
      "|    NULL|[Dominik Szajerma...|00964544-cbe2-4da...|         0|[3fcd7cdc-20e6-4e...|Fur Visualisation...|international con...|2014|\n",
      "|    NULL|[David Al-Dabass,...|00ba1eb4-d1aa-458...|         0|                NULL|Simulation of a v...|                    |1995|\n",
      "|    NULL|[Reiko Heckel, To...|00dc2bba-3237-4d4...|        50|                NULL|Software Evolutio...|Electronic Notes ...|2003|\n",
      "|    NULL|[Milos Zelezný, P...|00e02aeb-b424-4ca...|         7|                NULL|Design of an audi...|conference of the...|2002|\n",
      "|    NULL|[Abdullah Alsubai...|00e3940b-201b-40b...|         6|[0293844d-7244-45...|A Platform for Di...|                    |2013|\n",
      "|    NULL|[Tien Ho-Phuoc, A...|00fd4f6f-c7a1-49a...|         3|                NULL|A COMPUTATIONAL S...|international con...|2009|\n",
      "|    NULL|[Marco Baroni, Fr...|01082175-d4e3-456...|       101|                NULL|Cleaneval: a Comp...|language resource...|2008|\n",
      "|    NULL|[Nebojsa M. Ralev...|010fda73-e708-489...|         0|[17d92951-cd71-49...|Algorithms for th...|                    |2007|\n",
      "|    NULL|[William Perrizo,...|01103168-55d8-42b...|         0|                NULL|Vertical Database...|                    |2005|\n",
      "|    NULL|[Norihiko Moriwak...|014df191-abb2-44c...|         0|[08704b8d-70a1-44...|Sensor-Data-Drive...|                    |2014|\n",
      "|    NULL|[Prudence W. H. W...|01546472-0ac9-47f...|         6|[08784bb3-09b9-4b...|An 8/3 Lower Boun...|international sym...|2012|\n",
      "|    NULL|[Olivia Mendoza, ...|016eba4d-f912-492...|        50|[159fdfd2-921e-42...|Comparison of Fuz...|      soft computing|2010|\n",
      "|    NULL|[Martine Collard,...|01733dc2-4910-40d...|         3|[0102f234-e30b-4b...|Extracted Knowled...|research challeng...|2007|\n",
      "|    NULL|[Yosuke Furukawa,...|019a51de-5136-41c...|        50|                NULL|Context Dependent...|Journal of Advanc...|2007|\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.describe().show()\n",
    "df.describe().toPandas()\n",
    "# Check for missing values (excluding `isnan`)\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "df.select(\"n_citation\").describe().show()\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecbe6b-1eed-46ed-89f8-f10bd399cbbb",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe239df3-6de6-4f89-91fe-82cd2d992dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|language|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|      en|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|      en|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|      en|\n",
      "|Recent achievemen...|[Ankita Brahmacha...|00a119c4-d367-460...|         0|[84d47128-58d0-41...|Identifying Psych...|                    |2013|      en|\n",
      "|Recently, Bridges...|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|        50|                  []|Multisymplectic S...|international con...|2002|      en|\n",
      "|Most applications...|[Patrick Cousot, ...|00c85316-bddf-4bc...|        50|[6e8a3ec3-9a99-4f...|Relational Abstra...|                    |1991|      en|\n",
      "|Three speech trai...|[Minoru Shigenaga...|00ca027b-5174-40f...|         0|                NULL|Speech training s...|international joi...|1979|      en|\n",
      "|This paper focuse...|[Efthymios Alepis...|00dd5ece-1339-4cb...|        50|[522bea13-bd45-49...|Knowledge Enginee...|joint conference ...|2008|      en|\n",
      "|Embedded systems ...|[Matias Madou, Bj...|01047814-b615-444...|        50|                NULL|Link-time compact...|european symposiu...|2004|      en|\n",
      "|Xax is a browser ...|[John R. Douceur,...|010d4ce9-0279-416...|        50|[0abc9de7-e047-44...|Leveraging legacy...|operating systems...|2008|      en|\n",
      "|In recent years, ...|[Hiroshi Furukawa...|010d9907-45ef-459...|         7|[b2f0e0d3-0071-40...|A pedestrian navi...|international con...|2013|      en|\n",
      "|Previous language...|[Carmen Fernández...|012b88ae-a763-45d...|        50|[00515e82-3da6-49...|Word pairs in lan...|                    |2004|      en|\n",
      "|Spatial encryptio...|[Michel Abdalla, ...|016a9a21-e882-4cd...|        50|[3140c9ba-8d98-42...|Leakage-Resilient...|                    |2012|      en|\n",
      "|In system operati...|[Mark Burgess, Al...|01705f09-d395-4a0...|         8|[363278e2-6c9d-45...|On system rollbac...|The Journal of Lo...|2011|      en|\n",
      "|Business strategy...|[Constantinos Gia...|01b6f2ca-3903-419...|        12|[0ce446cf-4f4f-49...|Model-Driven Stra...|                    |2012|      en|\n",
      "|FTP Mirror Tracke...|[Alexei Novikov, ...|01ccb92f-46f1-400...|         0|                  []|FTP Mirror Tracke...|usenix large inst...|2000|      en|\n",
      "|There are a numbe...|[Theresa Beauboue...|01edeac9-cd8b-46f...|         0|[09b666de-6279-48...|Information Syste...|                    |2014|      en|\n",
      "|Breast cancer is ...|[Bartosz Krawczyk...|0265aea8-65f3-4f4...|        50|[16f0341b-538d-40...|Breast Cancer Ide...|                    |2013|      en|\n",
      "|The development o...|[J. Niblock, Jian...|028d37c8-b571-41b...|         0|[8fc06656-ed2a-46...|Automated Object ...|international con...|2008|      en|\n",
      "|Quality specified...|[Qiong Liu, You Y...|02a5e8a8-061e-4c1...|         3|[04f8d077-765a-41...|Quality Assessmen...|conference on mul...|2013|      en|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter only English documents\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "df = df.filter(df.language == 'en')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0831673f-7879-49cd-80c3-9fd96763d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8909ec04-a86c-4067-8f78-a865aa52ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_abstract\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a42bc8c-d4f1-4f3f-8369-425021008da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59921d34-5d8c-47f8-a576-7b110d9d6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_filtered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d03d68-2c4a-4c68-bda5-fee948490998",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0804d576-3882-475c-aed0-bf9f1a316a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2VecModel' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m Word2Vec(vectorSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, minCount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m word2vec\u001b[38;5;241m.\u001b[39mfit(df_filtered)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n\u001b[1;32m      5\u001b[0m df_vectorized \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df_filtered)\n\u001b[1;32m      6\u001b[0m df_vectorized\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecModel' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=10, minCount=1, inputCol=\"filtered_words\", outputCol=\"word2vec_features\")\n",
    "model = word2vec.fit(df_filtered)\n",
    "df_vectorized = model.transform(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86fc5262-4383-4412-953d-c0192f14a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features column we use later\n",
    "df_vectorized = df_vectorized.select(\"id\", \"venue\", \"word2vec_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b40a4be-80f9-4378-9845-bbee0610f1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07272269 -0.028699    0.04535926 ...  0.05457747  0.01635661\n",
      "   0.0098603 ]\n",
      " [ 0.04013565 -0.01322869  0.02499934 ...  0.04030106  0.00792694\n",
      "   0.00981932]\n",
      " [ 0.03491094 -0.01544054  0.01589792 ...  0.0321104   0.00716053\n",
      "   0.00981559]\n",
      " ...\n",
      " [ 0.02747165 -0.00623152  0.01504266 ...  0.0201032   0.01289248\n",
      "   0.00975341]\n",
      " [ 0.03609042 -0.02056411  0.0190711  ...  0.04708068  0.0254382\n",
      "  -0.00260146]\n",
      " [ 0.03789394 -0.01293251  0.02585634 ...  0.04101831  0.00868564\n",
      "   0.01077192]]\n"
     ]
    }
   ],
   "source": [
    "# Define UDF for converting sparse to dense vectors\n",
    "def to_dense(v):\n",
    "    if isinstance(v, SparseVector):\n",
    "        return DenseVector(v.toArray())\n",
    "    elif isinstance(v, DenseVector):\n",
    "        return v\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported vector type\")\n",
    "\n",
    "to_dense_udf = udf(to_dense, VectorUDT())\n",
    "# Apply UDF to convert features to dense vectors and extract feature column\n",
    "df_dense = df_vectorized.withColumn('dense_features', to_dense_udf(col('word2vec_features')))\n",
    "df_dense = df_dense.select(\"id\", \"venue\", \"dense_features\")\n",
    "\n",
    "# Convert the Spark DataFrame to Pandas DataFrame\n",
    "df_dense_pd = df_dense.toPandas()\n",
    "features = np.array(df_dense_pd['dense_features'].tolist())\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bd06508-2524-4638-9636-a33aa4796320",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1973a056-02b8-4b98-b64b-f21a1ae90280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.17582771e+00  7.09799334e-01  1.93215792e+00 ...  1.58070608e+00\n",
      "   1.19619176e-01 -6.99988563e-01]\n",
      " [-1.07988216e+00 -6.49228112e-01  5.51096216e-02 ...  3.65257200e-01\n",
      "  -1.30343588e-01 -3.60430282e-03]\n",
      " [-2.64669466e+00 -1.80007820e+00  5.22300645e-01 ...  3.02280402e-02\n",
      "  -6.53033704e-01  5.57475707e-01]\n",
      " ...\n",
      " [-4.46005973e+00 -1.97409329e-01 -1.05288617e+00 ...  6.93679857e-01\n",
      "   5.81926905e-01  6.48091768e-03]\n",
      " [-8.06647360e-01  4.34452754e+00 -1.06991095e+00 ...  2.49092950e-01\n",
      "  -7.84485784e-01 -2.10754054e-01]\n",
      " [-1.61166735e+00 -5.75986397e-01 -1.12374250e+00 ...  1.01569546e+00\n",
      "  -2.34936365e-01 -1.13685209e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data for PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA to reduce dimensions while keeping 95% of the variance\n",
    "pca = PCA(n_components=0.95)\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "print(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4ed7f-b388-452d-93d4-2eda1cd9b2ca",
   "metadata": {},
   "source": [
    "## Ploting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5da554-fd55-420f-b676-30cfa4a2582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "wcss = []\n",
    "for i in range(1, 21):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(reduced_data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 21), wcss, 'bo-')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squares')\n",
    "plt.show()\n",
    "\n",
    "optimal_k = 100\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(reduced_data)\n",
    "df_dense_pd['Cluster'] = clusters\n",
    "print(df_dense_pd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3cd8f-1328-42e7-85bb-99353d3273f6",
   "metadata": {},
   "source": [
    "## Recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a6460-97f9-4a62-a6a1-ea5435e372ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommending papers\n",
    "def recommend_papers(input_title, N):\n",
    "    # Vectorize the input title using the Word2Vec model\n",
    "    input_vector = model.transform(spark.createDataFrame([(input_title.split(),)], [\"filtered_words\"])).select(\"word2vec_features\").collect()[0][0]\n",
    "    input_vector = np.array(input_vector).reshape(1, -1)\n",
    "    \n",
    "    # Predict the cluster for the input title\n",
    "    cluster_label = km_model.predict(input_vector)[0]\n",
    "    \n",
    "    # Get indices of papers in the same cluster\n",
    "    cluster_indices = df_vectorized.filter(df_vectorized['id'] == cluster_label).select(\"id\").collect()\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cluster_indices = [index[\"id\"] for index in cluster_indices]\n",
    "    similarities = cosine_similarity(input_vector, word2vec_features[cluster_indices].reshape(-1, word2vec_features.shape[-1])).flatten()\n",
    "    \n",
    "    # Get top N similar papers\n",
    "    similar_indices = np.argsort(similarities)[-N:][::-1]\n",
    "\n",
    "    recommended_papers = [cluster_paper_titles[i] for i in similar_indices]\n",
    "    return recommended_papers\n",
    "# Example usage\n",
    "input_title = \"Deep Learning for Natural Language Processing\"\n",
    "N = 3\n",
    "recommended_papers = recommend_papers(\"Urban\", N)\n",
    "# print(\"Recommended papers:\")\n",
    "# for paper in recommended_papers:\n",
    "#     print(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e279493-ed50-4dc8-a50a-4eb08c159d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
