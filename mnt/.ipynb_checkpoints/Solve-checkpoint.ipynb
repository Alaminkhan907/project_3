{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eed696d0-5905-4f84-ab70-da12c3bf3537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in /opt/conda/lib/python3.11/site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from langid) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "#Importing all\n",
    "!pip install langid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "import numpy as np\n",
    "import langid\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.ml.feature import StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35293e97-8df2-4583-9248-ce50f76fa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project 3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16c479b8-f7ee-4040-a507-71555fa1ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dblp.v10.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -u dblp.v10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b412ae2-3ba2-41e9-9261-614b0e051383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45ce2893-0627-4e58-a576-d60c7cef35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|Based on biologic...|[Guoping Pang, La...|4aa69add-3978-480...|         8|[04754a28-6bf4-4d...|Dynamic analysis ...|Mathematics and C...|2008|\n",
      "|In this paper, a ...|[S. Ben Jabra, Ez...|4ab3735c-80f1-472...|        50|[09cb2d7d-47d1-4a...|A new approach of...|international sym...|2008|\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|AdaBoost algorith...|[Zheng Xu, Runbin...|001eef4f-1d00-4ae...|         0|[0a11984c-ab6e-4b...|A Heterogeneous S...|high performance ...|2016|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "\n",
      "Number of records: 4\n"
     ]
    }
   ],
   "source": [
    "# Show the schema to understand the structure\n",
    "df.printSchema()\n",
    "# Display a sample of the data\n",
    "df.show(5)\n",
    "# Count the number of records\n",
    "print(f\"Number of records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df179595-ce55-43bc-944f-effc8571d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|summary|            abstract|                  id|       n_citation|               title|               venue|              year|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|  count|                   4|                   4|                4|                   4|                   4|                 4|\n",
      "|   mean|                NULL|                NULL|             14.5|                NULL|                NULL|           2011.25|\n",
      "| stddev|                NULL|                NULL|23.96525262402492|                NULL|                NULL|3.9475730941089733|\n",
      "|    min|AdaBoost algorith...|00127ee2-cb05-48c...|                0|A Heterogeneous S...|Mathematics and C...|              2008|\n",
      "|    max|The purpose of th...|4ab3735c-80f1-472...|               50|Preliminary Desig...|international sym...|              2016|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|       0|      0|  0|         0|         0|    0|    0|   0|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|       n_citation|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|             14.5|\n",
      "| stddev|23.96525262402492|\n",
      "|    min|                0|\n",
      "|    max|               50|\n",
      "+-------+-----------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Check for missing values (excluding `isnan`)\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Distribution of citations\n",
    "df.select(\"n_citation\").describe().show()\n",
    "\n",
    "# Check for null abstracts and titles\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7855f3f-7a7b-4d23-bf45-4c4981e11071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language using langid\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Add a new column for language detection\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "\n",
    "# Filter only English documents\n",
    "df = df.filter(df.language == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ea38aff-6902-4413-87ae-bbe86f4ce128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', \n",
    "                     'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', \n",
    "                     'CZI', 'www']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea663933-fdbb-4ca7-9e30-7500a98a7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed2d3f6f-b148-45ec-a1a4-713a229eb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "df_tokenized = df_cleaned.withColumn(\"tokenized_abstract\", split(col(\"cleaned_abstract\"), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eeb89221-56de-41ac-acfb-bfb28aa8ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokenized_abstract\", outputCol=\"filtered_abstract\", \n",
    "                           stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b340bef-4247-424e-8e9a-99568fdf5cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |cleaned_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |tokenized_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |filtered_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Based on biological control strategy in pest management, we construct and investigate a pest-epidemic model with impulsive control, i.e., periodic spraying microbial pesticide and releasing infected pests at different fixed moments. By using Floquet theorem and comparison theorem, we prove that the pest-eradication periodic solution is globally asymptotically stable when the impulsive period @t is less than the critical value @t\"m\"a\"x\"@?. Otherwise, the system can be permanent. Moreover, numerical results clearly show with the increase of the impulsive period @t, the system exhibits a wide variety of dynamic behaviors including a sequence of direct and inverse cascade of periodic-doubling, symmetry-breaking pitchfork bifurcation, chaos and non-unique dynamics, which implies that the impulsive effect makes the dynamic behavior of the system more complex.                                                                                                                                                                                                                                                                                                                                |based on biological control strategy in pest management we construct and investigate a pestepidemic model with impulsive control ie periodic spraying microbial pesticide and releasing infected pests at different fixed moments by using floquet theorem and comparison theorem we prove that the pesteradication periodic solution is globally asymptotically stable when the impulsive period t is less than the critical value tmax otherwise the system can be permanent moreover numerical results clearly show with the increase of the impulsive period t the system exhibits a wide variety of dynamic behaviors including a sequence of direct and inverse cascade of periodicdoubling symmetrybreaking pitchfork bifurcation chaos and nonunique dynamics which implies that the impulsive effect makes the dynamic behavior of the system more complex                                                                                                                                                                                                                                                                                                                                  |[based, on, biological, control, strategy, in, pest, management, we, construct, and, investigate, a, pestepidemic, model, with, impulsive, control, ie, periodic, spraying, microbial, pesticide, and, releasing, infected, pests, at, different, fixed, moments, by, using, floquet, theorem, and, comparison, theorem, we, prove, that, the, pesteradication, periodic, solution, is, globally, asymptotically, stable, when, the, impulsive, period, t, is, less, than, the, critical, value, tmax, otherwise, the, system, can, be, permanent, moreover, numerical, results, clearly, show, with, the, increase, of, the, impulsive, period, t, the, system, exhibits, a, wide, variety, of, dynamic, behaviors, including, a, sequence, of, direct, and, inverse, cascade, of, periodicdoubling, symmetrybreaking, pitchfork, bifurcation, chaos, and, nonunique, dynamics, which, implies, that, the, impulsive, effect, makes, the, dynamic, behavior, of, the, system, more, complex]                                                                                                                                                                                                                                                                                                                                                                           |[based, biological, control, strategy, pest, management, construct, investigate, pestepidemic, model, impulsive, control, ie, periodic, spraying, microbial, pesticide, releasing, infected, pests, different, fixed, moments, floquet, theorem, comparison, theorem, prove, pesteradication, periodic, solution, globally, asymptotically, stable, impulsive, period, less, critical, value, tmax, otherwise, system, permanent, moreover, numerical, results, clearly, show, increase, impulsive, period, system, exhibits, wide, variety, dynamic, behaviors, including, sequence, direct, inverse, cascade, periodicdoubling, symmetrybreaking, pitchfork, bifurcation, chaos, nonunique, dynamics, implies, impulsive, effect, makes, dynamic, behavior, system, complex]                                                                                                                                                                                                                                                                                                                             |\n",
      "|In this paper, a robust 3D triangular mesh watermarking algorithm based on 3D segmentation is proposed. In this algorithm three classes of watermarking are combined. First, we segment the original image to many different regions. Then we mark every type of region with the corresponding algorithm based on their curvature value. The experiments show that our watermarking is robust against numerous attacks including RST transformations, smoothing, additive random noise, cropping, simplification and remeshing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |in this paper a robust 3d triangular mesh watermarking algorithm based on 3d segmentation is proposed in this algorithm three classes of watermarking are combined first we segment the original image to many different regions then we mark every type of region with the corresponding algorithm based on their curvature value the experiments show that our watermarking is robust against numerous attacks including rst transformations smoothing additive random noise cropping simplification and remeshing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |[in, this, paper, a, robust, 3d, triangular, mesh, watermarking, algorithm, based, on, 3d, segmentation, is, proposed, in, this, algorithm, three, classes, of, watermarking, are, combined, first, we, segment, the, original, image, to, many, different, regions, then, we, mark, every, type, of, region, with, the, corresponding, algorithm, based, on, their, curvature, value, the, experiments, show, that, our, watermarking, is, robust, against, numerous, attacks, including, rst, transformations, smoothing, additive, random, noise, cropping, simplification, and, remeshing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |[paper, robust, 3d, triangular, mesh, watermarking, algorithm, based, 3d, segmentation, proposed, algorithm, three, classes, watermarking, combined, first, segment, original, image, many, different, regions, mark, every, type, region, corresponding, algorithm, based, curvature, value, experiments, show, watermarking, robust, numerous, attacks, including, rst, transformations, smoothing, additive, random, noise, cropping, simplification, remeshing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|The purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net- works. More specifically, we focus on the basic principles of network proto- cols as the aim to develop our learning tool. Our tool gives students hands-on experience to help understand the basic principles of network protocols.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |the purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net works more specifically we focus on the basic principles of network proto cols as the aim to develop our learning tool our tool gives students handson experience to help understand the basic principles of network protocols                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |[the, purpose, of, this, study, is, to, develop, a, learning, tool, for, high, school, students, studying, the, scientific, aspects, of, information, and, communication, net, works, more, specifically, we, focus, on, the, basic, principles, of, network, proto, cols, as, the, aim, to, develop, our, learning, tool, our, tool, gives, students, handson, experience, to, help, understand, the, basic, principles, of, network, protocols]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |[purpose, study, develop, learning, tool, high, school, students, studying, scientific, aspects, information, communication, net, works, specifically, focus, basic, principles, network, proto, cols, aim, develop, learning, tool, tool, gives, students, handson, experience, help, understand, basic, principles, network, protocols]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|AdaBoost algorithm based on Haar-like features can achieves high accuracy (above 95%) in object detection. Meanwhile massive computing power is needed to implement the cascaded classifiers involved in AdaBoost detection. To solve this problem, several dedicated hardware solutions have been proposed for real-time applications. In this work, a novel heterogeneous architecture of an AdaBoost detector is presented. This architecture achieves higher performance while consuming fewer hardware resources. By combining an integrated ARM Cortex-A9 processor with a dedicated accelerator, this architecture can be configured to realize various objects detection by simply loading different parameters. 2-D parallelism is involved in accelerator unit combination which brings more flexibility. This scheme is implemented on Xilinx ZC702 platform, the experiment result shows that 40 QVGA frames per second can be achieved for real-time face detection. The accelerator achieves more than 13 times improvement over the OpenCV implementation on a standalone Cortex-A9 w.r.t execution speed. Meanwhile, the accelerator consumes 40% less FPGA hardware resources than the prior-art implementation.|adaboost algorithm based on haarlike features can achieves high accuracy above 95 in object detection meanwhile massive computing power is needed to implement the cascaded classifiers involved in adaboost detection to solve this problem several dedicated hardware solutions have been proposed for realtime applications in this work a novel heterogeneous architecture of an adaboost detector is presented this architecture achieves higher performance while consuming fewer hardware resources by combining an integrated arm cortexa9 processor with a dedicated accelerator this architecture can be configured to realize various objects detection by simply loading different parameters 2d parallelism is involved in accelerator unit combination which brings more flexibility this scheme is implemented on xilinx zc702 platform the experiment result shows that 40 qvga frames per second can be achieved for realtime face detection the accelerator achieves more than 13 times improvement over the opencv implementation on a standalone cortexa9 wrt execution speed meanwhile the accelerator consumes 40 less fpga hardware resources than the priorart implementation|[adaboost, algorithm, based, on, haarlike, features, can, achieves, high, accuracy, above, 95, in, object, detection, meanwhile, massive, computing, power, is, needed, to, implement, the, cascaded, classifiers, involved, in, adaboost, detection, to, solve, this, problem, several, dedicated, hardware, solutions, have, been, proposed, for, realtime, applications, in, this, work, a, novel, heterogeneous, architecture, of, an, adaboost, detector, is, presented, this, architecture, achieves, higher, performance, while, consuming, fewer, hardware, resources, by, combining, an, integrated, arm, cortexa9, processor, with, a, dedicated, accelerator, this, architecture, can, be, configured, to, realize, various, objects, detection, by, simply, loading, different, parameters, 2d, parallelism, is, involved, in, accelerator, unit, combination, which, brings, more, flexibility, this, scheme, is, implemented, on, xilinx, zc702, platform, the, experiment, result, shows, that, 40, qvga, frames, per, second, can, be, achieved, for, realtime, face, detection, the, accelerator, achieves, more, than, 13, times, improvement, over, the, opencv, implementation, on, a, standalone, cortexa9, wrt, execution, speed, meanwhile, the, accelerator, consumes, 40, less, fpga, hardware, resources, than, the, priorart, implementation]|[adaboost, algorithm, based, haarlike, features, achieves, high, accuracy, 95, object, detection, meanwhile, massive, computing, power, needed, implement, cascaded, classifiers, involved, adaboost, detection, solve, problem, several, dedicated, hardware, solutions, proposed, realtime, applications, work, novel, heterogeneous, architecture, adaboost, detector, presented, architecture, achieves, higher, performance, consuming, fewer, hardware, resources, combining, integrated, arm, cortexa9, processor, dedicated, accelerator, architecture, configured, realize, various, objects, detection, simply, loading, different, parameters, 2d, parallelism, involved, accelerator, unit, combination, brings, flexibility, scheme, implemented, xilinx, zc702, platform, experiment, result, shows, 40, qvga, frames, per, second, achieved, realtime, face, detection, accelerator, achieves, 13, times, improvement, opencv, implementation, standalone, cortexa9, wrt, execution, speed, meanwhile, accelerator, consumes, 40, less, fpga, hardware, resources, priorart, implementation]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the cleaned dataframe\n",
    "df_filtered.select(\"abstract\", \"cleaned_abstract\", \"tokenized_abstract\", \"filtered_abstract\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1b36a5a-d6c0-4e5a-8034-47d1b9886237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|language|    cleaned_abstract|  tokenized_abstract|   filtered_abstract|         rawFeatures|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Based on biologic...|[Guoping Pang, La...|4aa69add-3978-480...|         8|[04754a28-6bf4-4d...|Dynamic analysis ...|Mathematics and C...|2008|      en|based on biologic...|[based, on, biolo...|[based, biologica...|(20000,[28,42,274...|\n",
      "|In this paper, a ...|[S. Ben Jabra, Ez...|4ab3735c-80f1-472...|        50|[09cb2d7d-47d1-4a...|A new approach of...|international sym...|2008|      en|in this paper a r...|[in, this, paper,...|[paper, robust, 3...|(20000,[78,274,46...|\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|      en|the purpose of th...|[the, purpose, of...|[purpose, study, ...|(20000,[1072,1241...|\n",
      "|AdaBoost algorith...|[Zheng Xu, Runbin...|001eef4f-1d00-4ae...|         0|[0a11984c-ab6e-4b...|A Heterogeneous S...|high performance ...|2016|      en|adaboost algorith...|[adaboost, algori...|[adaboost, algori...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_abstract\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "df_featurized = hashingTF.transform(df_filtered)\n",
    "# df_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "593dfa35-f111-431c-bb48-ad6b1dd73b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|language|    cleaned_abstract|  tokenized_abstract|   filtered_abstract|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Based on biologic...|[Guoping Pang, La...|4aa69add-3978-480...|         8|[04754a28-6bf4-4d...|Dynamic analysis ...|Mathematics and C...|2008|      en|based on biologic...|[based, on, biolo...|[based, biologica...|(20000,[28,42,274...|(20000,[28,42,274...|\n",
      "|In this paper, a ...|[S. Ben Jabra, Ez...|4ab3735c-80f1-472...|        50|[09cb2d7d-47d1-4a...|A new approach of...|international sym...|2008|      en|in this paper a r...|[in, this, paper,...|[paper, robust, 3...|(20000,[78,274,46...|(20000,[78,274,46...|\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|      en|the purpose of th...|[the, purpose, of...|[purpose, study, ...|(20000,[1072,1241...|(20000,[1072,1241...|\n",
      "|AdaBoost algorith...|[Zheng Xu, Runbin...|001eef4f-1d00-4ae...|         0|[0a11984c-ab6e-4b...|A Heterogeneous S...|high performance ...|2016|      en|adaboost algorith...|[adaboost, algori...|[adaboost, algori...|(20000,[193,274,2...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_featurized)\n",
    "df_vectorized = idf_model.transform(df_featurized)\n",
    "df_vectorized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f13acb9-a1c5-4ca9-9af2-2ee91133335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|               title|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|4aa69add-3978-480...|Dynamic analysis ...|(20000,[28,42,274...|\n",
      "|4ab3735c-80f1-472...|A new approach of...|(20000,[78,274,46...|\n",
      "|00127ee2-cb05-48c...|Preliminary Desig...|(20000,[1072,1241...|\n",
      "|001eef4f-1d00-4ae...|A Heterogeneous S...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "df_vectorized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54347d4c-a4aa-44be-96a8-68852cf3bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: string, title: string, features: vector]\n",
      "Sampled DataFrame count: 3\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|               title|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|4aa69add-3978-480...|Dynamic analysis ...|(20000,[28,42,274...|\n",
      "|00127ee2-cb05-48c...|Preliminary Desig...|(20000,[1072,1241...|\n",
      "|001eef4f-1d00-4ae...|A Heterogeneous S...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clustering \n",
    "\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.count() == 0 and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.count() == 0:\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "# Sample the data with initial fraction 0.1\n",
    "df_sampled = sample_data(df_vectorized, 0.1)\n",
    "print(df_sampled)\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "df_sampled.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4618632-af3b-4adc-b8c3-f0f13ad40073",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA.__init__() got an unexpected keyword argument 'n_components'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply PCA to reduce dimensions (e.g., to 20 components)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mPCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit(df_sampled)\n\u001b[1;32m      4\u001b[0m df_pca \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(df_sampled)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA.__init__() got an unexpected keyword argument 'n_components'"
     ]
    }
   ],
   "source": [
    "# Apply PCA to reduce dimensions (e.g., to 20 components)\n",
    "pca = PCA(k=20, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_sampled)\n",
    "df_pca = pca_model.transform(df_sampled)\n",
    "print(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0fa8e1b-69fb-411d-9f59-75808d4c827a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'coalesce'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dimensionality Reduction with PCA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit(\u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m(\u001b[38;5;241m1000\u001b[39m))\u001b[38;5;241m.\u001b[39mtransform(df_tfidf)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'coalesce'"
     ]
    }
   ],
   "source": [
    "# Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components = 0.95)\n",
    "df_pca = pca.fit(pca.coalesce(1000)).transform(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289ef63-7603-44ca-81fc-52e919dc2d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
