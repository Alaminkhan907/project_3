{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed696d0-5905-4f84-ab70-da12c3bf3537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langid\n",
      "  Using cached langid-1.1.6-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from langid) (1.26.4)\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n"
     ]
    }
   ],
   "source": [
    "#Importing all\n",
    "!pip install langid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "import numpy as np\n",
    "import langid\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.ml.feature import StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35293e97-8df2-4583-9248-ce50f76fa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Project 3\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b633be-3fc7-45ef-ba49-09953733b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession with the required configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.shuffle.partitions\" ,10) \\\n",
    "    .config(\"spark.executor.memory\" , \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Project 3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c479b8-f7ee-4040-a507-71555fa1ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dblp.v10.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -u dblp.v10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b412ae2-3ba2-41e9-9261-614b0e051383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ce2893-0627-4e58-a576-d60c7cef35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|Based on biologic...|[Guoping Pang, La...|4aa69add-3978-480...|         8|[04754a28-6bf4-4d...|Dynamic analysis ...|Mathematics and C...|2008|\n",
      "|In this paper, a ...|[S. Ben Jabra, Ez...|4ab3735c-80f1-472...|        50|[09cb2d7d-47d1-4a...|A new approach of...|international sym...|2008|\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|AdaBoost algorith...|[Zheng Xu, Runbin...|001eef4f-1d00-4ae...|         0|[0a11984c-ab6e-4b...|A Heterogeneous S...|high performance ...|2016|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "\n",
      "Number of records: 4\n"
     ]
    }
   ],
   "source": [
    "# Show the schema\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Number of records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df179595-ce55-43bc-944f-effc8571d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|summary|            abstract|                  id|       n_citation|               title|               venue|              year|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "|  count|                   4|                   4|                4|                   4|                   4|                 4|\n",
      "|   mean|                NULL|                NULL|             14.5|                NULL|                NULL|           2011.25|\n",
      "| stddev|                NULL|                NULL|23.96525262402492|                NULL|                NULL|3.9475730941089733|\n",
      "|    min|AdaBoost algorith...|00127ee2-cb05-48c...|                0|A Heterogeneous S...|Mathematics and C...|              2008|\n",
      "|    max|The purpose of th...|4ab3735c-80f1-472...|               50|Preliminary Desig...|international sym...|              2016|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+------------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|       0|      0|  0|         0|         0|    0|    0|   0|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|       n_citation|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|             14.5|\n",
      "| stddev|23.96525262402492|\n",
      "|    min|                0|\n",
      "|    max|               50|\n",
      "+-------+-----------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Check for missing values (excluding `isnan`)\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Distribution of citations\n",
    "df.select(\"n_citation\").describe().show()\n",
    "\n",
    "# Check for null abstracts and titles\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7855f3f-7a7b-4d23-bf45-4c4981e11071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language using langid\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Add a new column for language detection\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "\n",
    "# Filter only English documents\n",
    "df = df.filter(df.language == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea38aff-6902-4413-87ae-bbe86f4ce128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', \n",
    "                     'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', \n",
    "                     'CZI', 'www']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea663933-fdbb-4ca7-9e30-7500a98a7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed2d3f6f-b148-45ec-a1a4-713a229eb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "df_tokenized = df_cleaned.withColumn(\"tokenized_abstract\", split(col(\"cleaned_abstract\"), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb89221-56de-41ac-acfb-bfb28aa8ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokenized_abstract\", outputCol=\"filtered_abstract\", \n",
    "                           stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b340bef-4247-424e-8e9a-99568fdf5cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |cleaned_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |tokenized_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |filtered_abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Based on biological control strategy in pest management, we construct and investigate a pest-epidemic model with impulsive control, i.e., periodic spraying microbial pesticide and releasing infected pests at different fixed moments. By using Floquet theorem and comparison theorem, we prove that the pest-eradication periodic solution is globally asymptotically stable when the impulsive period @t is less than the critical value @t\"m\"a\"x\"@?. Otherwise, the system can be permanent. Moreover, numerical results clearly show with the increase of the impulsive period @t, the system exhibits a wide variety of dynamic behaviors including a sequence of direct and inverse cascade of periodic-doubling, symmetry-breaking pitchfork bifurcation, chaos and non-unique dynamics, which implies that the impulsive effect makes the dynamic behavior of the system more complex.                                                                                                                                                                                                                                                                                                                                |based on biological control strategy in pest management we construct and investigate a pestepidemic model with impulsive control ie periodic spraying microbial pesticide and releasing infected pests at different fixed moments by using floquet theorem and comparison theorem we prove that the pesteradication periodic solution is globally asymptotically stable when the impulsive period t is less than the critical value tmax otherwise the system can be permanent moreover numerical results clearly show with the increase of the impulsive period t the system exhibits a wide variety of dynamic behaviors including a sequence of direct and inverse cascade of periodicdoubling symmetrybreaking pitchfork bifurcation chaos and nonunique dynamics which implies that the impulsive effect makes the dynamic behavior of the system more complex                                                                                                                                                                                                                                                                                                                                  |[based, on, biological, control, strategy, in, pest, management, we, construct, and, investigate, a, pestepidemic, model, with, impulsive, control, ie, periodic, spraying, microbial, pesticide, and, releasing, infected, pests, at, different, fixed, moments, by, using, floquet, theorem, and, comparison, theorem, we, prove, that, the, pesteradication, periodic, solution, is, globally, asymptotically, stable, when, the, impulsive, period, t, is, less, than, the, critical, value, tmax, otherwise, the, system, can, be, permanent, moreover, numerical, results, clearly, show, with, the, increase, of, the, impulsive, period, t, the, system, exhibits, a, wide, variety, of, dynamic, behaviors, including, a, sequence, of, direct, and, inverse, cascade, of, periodicdoubling, symmetrybreaking, pitchfork, bifurcation, chaos, and, nonunique, dynamics, which, implies, that, the, impulsive, effect, makes, the, dynamic, behavior, of, the, system, more, complex]                                                                                                                                                                                                                                                                                                                                                                           |[based, biological, control, strategy, pest, management, construct, investigate, pestepidemic, model, impulsive, control, ie, periodic, spraying, microbial, pesticide, releasing, infected, pests, different, fixed, moments, floquet, theorem, comparison, theorem, prove, pesteradication, periodic, solution, globally, asymptotically, stable, impulsive, period, less, critical, value, tmax, otherwise, system, permanent, moreover, numerical, results, clearly, show, increase, impulsive, period, system, exhibits, wide, variety, dynamic, behaviors, including, sequence, direct, inverse, cascade, periodicdoubling, symmetrybreaking, pitchfork, bifurcation, chaos, nonunique, dynamics, implies, impulsive, effect, makes, dynamic, behavior, system, complex]                                                                                                                                                                                                                                                                                                                             |\n",
      "|In this paper, a robust 3D triangular mesh watermarking algorithm based on 3D segmentation is proposed. In this algorithm three classes of watermarking are combined. First, we segment the original image to many different regions. Then we mark every type of region with the corresponding algorithm based on their curvature value. The experiments show that our watermarking is robust against numerous attacks including RST transformations, smoothing, additive random noise, cropping, simplification and remeshing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |in this paper a robust 3d triangular mesh watermarking algorithm based on 3d segmentation is proposed in this algorithm three classes of watermarking are combined first we segment the original image to many different regions then we mark every type of region with the corresponding algorithm based on their curvature value the experiments show that our watermarking is robust against numerous attacks including rst transformations smoothing additive random noise cropping simplification and remeshing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |[in, this, paper, a, robust, 3d, triangular, mesh, watermarking, algorithm, based, on, 3d, segmentation, is, proposed, in, this, algorithm, three, classes, of, watermarking, are, combined, first, we, segment, the, original, image, to, many, different, regions, then, we, mark, every, type, of, region, with, the, corresponding, algorithm, based, on, their, curvature, value, the, experiments, show, that, our, watermarking, is, robust, against, numerous, attacks, including, rst, transformations, smoothing, additive, random, noise, cropping, simplification, and, remeshing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |[paper, robust, 3d, triangular, mesh, watermarking, algorithm, based, 3d, segmentation, proposed, algorithm, three, classes, watermarking, combined, first, segment, original, image, many, different, regions, mark, every, type, region, corresponding, algorithm, based, curvature, value, experiments, show, watermarking, robust, numerous, attacks, including, rst, transformations, smoothing, additive, random, noise, cropping, simplification, remeshing]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|The purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net- works. More specifically, we focus on the basic principles of network proto- cols as the aim to develop our learning tool. Our tool gives students hands-on experience to help understand the basic principles of network protocols.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |the purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net works more specifically we focus on the basic principles of network proto cols as the aim to develop our learning tool our tool gives students handson experience to help understand the basic principles of network protocols                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |[the, purpose, of, this, study, is, to, develop, a, learning, tool, for, high, school, students, studying, the, scientific, aspects, of, information, and, communication, net, works, more, specifically, we, focus, on, the, basic, principles, of, network, proto, cols, as, the, aim, to, develop, our, learning, tool, our, tool, gives, students, handson, experience, to, help, understand, the, basic, principles, of, network, protocols]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |[purpose, study, develop, learning, tool, high, school, students, studying, scientific, aspects, information, communication, net, works, specifically, focus, basic, principles, network, proto, cols, aim, develop, learning, tool, tool, gives, students, handson, experience, help, understand, basic, principles, network, protocols]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|AdaBoost algorithm based on Haar-like features can achieves high accuracy (above 95%) in object detection. Meanwhile massive computing power is needed to implement the cascaded classifiers involved in AdaBoost detection. To solve this problem, several dedicated hardware solutions have been proposed for real-time applications. In this work, a novel heterogeneous architecture of an AdaBoost detector is presented. This architecture achieves higher performance while consuming fewer hardware resources. By combining an integrated ARM Cortex-A9 processor with a dedicated accelerator, this architecture can be configured to realize various objects detection by simply loading different parameters. 2-D parallelism is involved in accelerator unit combination which brings more flexibility. This scheme is implemented on Xilinx ZC702 platform, the experiment result shows that 40 QVGA frames per second can be achieved for real-time face detection. The accelerator achieves more than 13 times improvement over the OpenCV implementation on a standalone Cortex-A9 w.r.t execution speed. Meanwhile, the accelerator consumes 40% less FPGA hardware resources than the prior-art implementation.|adaboost algorithm based on haarlike features can achieves high accuracy above 95 in object detection meanwhile massive computing power is needed to implement the cascaded classifiers involved in adaboost detection to solve this problem several dedicated hardware solutions have been proposed for realtime applications in this work a novel heterogeneous architecture of an adaboost detector is presented this architecture achieves higher performance while consuming fewer hardware resources by combining an integrated arm cortexa9 processor with a dedicated accelerator this architecture can be configured to realize various objects detection by simply loading different parameters 2d parallelism is involved in accelerator unit combination which brings more flexibility this scheme is implemented on xilinx zc702 platform the experiment result shows that 40 qvga frames per second can be achieved for realtime face detection the accelerator achieves more than 13 times improvement over the opencv implementation on a standalone cortexa9 wrt execution speed meanwhile the accelerator consumes 40 less fpga hardware resources than the priorart implementation|[adaboost, algorithm, based, on, haarlike, features, can, achieves, high, accuracy, above, 95, in, object, detection, meanwhile, massive, computing, power, is, needed, to, implement, the, cascaded, classifiers, involved, in, adaboost, detection, to, solve, this, problem, several, dedicated, hardware, solutions, have, been, proposed, for, realtime, applications, in, this, work, a, novel, heterogeneous, architecture, of, an, adaboost, detector, is, presented, this, architecture, achieves, higher, performance, while, consuming, fewer, hardware, resources, by, combining, an, integrated, arm, cortexa9, processor, with, a, dedicated, accelerator, this, architecture, can, be, configured, to, realize, various, objects, detection, by, simply, loading, different, parameters, 2d, parallelism, is, involved, in, accelerator, unit, combination, which, brings, more, flexibility, this, scheme, is, implemented, on, xilinx, zc702, platform, the, experiment, result, shows, that, 40, qvga, frames, per, second, can, be, achieved, for, realtime, face, detection, the, accelerator, achieves, more, than, 13, times, improvement, over, the, opencv, implementation, on, a, standalone, cortexa9, wrt, execution, speed, meanwhile, the, accelerator, consumes, 40, less, fpga, hardware, resources, than, the, priorart, implementation]|[adaboost, algorithm, based, haarlike, features, achieves, high, accuracy, 95, object, detection, meanwhile, massive, computing, power, needed, implement, cascaded, classifiers, involved, adaboost, detection, solve, problem, several, dedicated, hardware, solutions, proposed, realtime, applications, work, novel, heterogeneous, architecture, adaboost, detector, presented, architecture, achieves, higher, performance, consuming, fewer, hardware, resources, combining, integrated, arm, cortexa9, processor, dedicated, accelerator, architecture, configured, realize, various, objects, detection, simply, loading, different, parameters, 2d, parallelism, involved, accelerator, unit, combination, brings, flexibility, scheme, implemented, xilinx, zc702, platform, experiment, result, shows, 40, qvga, frames, per, second, achieved, realtime, face, detection, accelerator, achieves, 13, times, improvement, opencv, implementation, standalone, cortexa9, wrt, execution, speed, meanwhile, accelerator, consumes, 40, less, fpga, hardware, resources, priorart, implementation]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the cleaned dataframe\n",
    "df_filtered.select(\"abstract\", \"cleaned_abstract\", \"tokenized_abstract\", \"filtered_abstract\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1b36a5a-d6c0-4e5a-8034-47d1b9886237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_abstract\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "df_featurized = hashingTF.transform(df_filtered)\n",
    "# df_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "593dfa35-f111-431c-bb48-ad6b1dd73b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|language|    cleaned_abstract|  tokenized_abstract|   filtered_abstract|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Based on biologic...|[Guoping Pang, La...|4aa69add-3978-480...|         8|[04754a28-6bf4-4d...|Dynamic analysis ...|Mathematics and C...|2008|      en|based on biologic...|[based, on, biolo...|[based, biologica...|(20000,[28,42,274...|(20000,[28,42,274...|\n",
      "|In this paper, a ...|[S. Ben Jabra, Ez...|4ab3735c-80f1-472...|        50|[09cb2d7d-47d1-4a...|A new approach of...|international sym...|2008|      en|in this paper a r...|[in, this, paper,...|[paper, robust, 3...|(20000,[78,274,46...|(20000,[78,274,46...|\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|      en|the purpose of th...|[the, purpose, of...|[purpose, study, ...|(20000,[1072,1241...|(20000,[1072,1241...|\n",
      "|AdaBoost algorith...|[Zheng Xu, Runbin...|001eef4f-1d00-4ae...|         0|[0a11984c-ab6e-4b...|A Heterogeneous S...|high performance ...|2016|      en|adaboost algorith...|[adaboost, algori...|[adaboost, algori...|(20000,[193,274,2...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_featurized)\n",
    "df_vectorized = idf_model.transform(df_featurized)\n",
    "df_vectorized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f13acb9-a1c5-4ca9-9af2-2ee91133335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|               title|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|4aa69add-3978-480...|Dynamic analysis ...|(20000,[28,42,274...|\n",
      "|4ab3735c-80f1-472...|A new approach of...|(20000,[78,274,46...|\n",
      "|00127ee2-cb05-48c...|Preliminary Desig...|(20000,[1072,1241...|\n",
      "|001eef4f-1d00-4ae...|A Heterogeneous S...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "df_vectorized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54347d4c-a4aa-44be-96a8-68852cf3bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: string, title: string, features: vector]\n",
      "Sampled DataFrame count: 3\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|               title|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|4aa69add-3978-480...|Dynamic analysis ...|(20000,[28,42,274...|\n",
      "|00127ee2-cb05-48c...|Preliminary Desig...|(20000,[1072,1241...|\n",
      "|001eef4f-1d00-4ae...|A Heterogeneous S...|(20000,[193,274,2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clustering \n",
    "\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.count() == 0 and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.count() == 0:\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "# Sample the data with initial fraction 0.1\n",
    "df_sampled = sample_data(df_vectorized, 0.1)\n",
    "print(df_sampled)\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "df_sampled.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c214f8c7-df05-4806-90cc-1db6cc8e089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sampled is a Spark DataFrame\n",
      "df_sampled has a column named 'features'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Check if df_sampled is a Spark DataFrame\n",
    "if isinstance(df_sampled, DataFrame):\n",
    "    print(\"df_sampled is a Spark DataFrame\")\n",
    "else:\n",
    "    print(\"df_sampled is not a Spark DataFrame\")\n",
    "\n",
    "# Check if df_sampled has a column named \"features\"\n",
    "if \"features\" in df_sampled.columns:\n",
    "    print(\"df_sampled has a column named 'features'\")\n",
    "else:\n",
    "    print(\"df_sampled does not have a column named 'features'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac73aa44-69f3-481d-b38f-adfecf27d1bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Cannot aggregate object of size 1600080000 Bytes, as it's bigger than maxResultSize (1073741824 Bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m df_sampled_repartitioned \u001b[38;5;241m=\u001b[39m df_sampled\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Fit the PCA model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_sampled_repartitioned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Transform the data using the PCA model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m df_pca \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(df_sampled_repartitioned)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Cannot aggregate object of size 1600080000 Bytes, as it's bigger than maxResultSize (1073741824 Bytes)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "\n",
    "# Create a PCA object with k=20, inputCol=\"features\", outputCol=\"pca_features\"\n",
    "pca = PCA(k=20, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Repartition the data to increase parallelism\n",
    "df_sampled_repartitioned = df_sampled.repartition(500)\n",
    "\n",
    "# Fit the PCA model\n",
    "pca_model = pca.fit(df_sampled_repartitioned)\n",
    "\n",
    "# Transform the data using the PCA model\n",
    "df_pca = pca_model.transform(df_sampled_repartitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4618632-af3b-4adc-b8c3-f0f13ad40073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply PCA to reduce dimensions (e.g., to 20 components)\n",
    "# pca = PCA(k=20, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "# pca_model = pca.fit(df_sampled)\n",
    "# df_pca = pca_model.transform(df_sampled)\n",
    "# print(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd85b3-edeb-402d-bd9d-18bfced22e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec926e3-7280-41d3-97eb-ce1fadb4d94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289ef63-7603-44ca-81fc-52e919dc2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method to Find Optimal K\n",
    "costs = []\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = kmeans.fit(df_pca)\n",
    "    costs.append(model.summary.trainingCost)\n",
    "\n",
    "plt.plot(range(2, 21), costs, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af23dfb-5d97-43a7-9216-6a7aaf84b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Clustering (using the optimal k from the elbow method)\n",
    "optimal_k = 10  # Example optimal k value\n",
    "kmeans = KMeans(k=optimal_k, seed=1, featuresCol=\"pca_features\")\n",
    "model = kmeans.fit(df_pca)\n",
    "df_clustered = model.transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a17f10-e99e-4ae8-8980-5e41bd0e0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Engine Function\n",
    "def recommend_papers(title, top_n=5):\n",
    "    paper_cluster = df_clustered.filter(col(\"title\") == title).select(\"prediction\").first()[0]\n",
    "    cluster_papers = df_clustered.filter(col(\"prediction\") == paper_cluster)\n",
    "    paper_features = df_clustered.filter(col(\"title\") == title).select(\"features\").first()[0]\n",
    "\n",
    "    cluster_papers = cluster_papers.withColumn(\"similarity\", cosine_similarity_udf(cluster_papers.features, array([paper_features])))\n",
    "    recommendations = cluster_papers.orderBy(\"similarity\", ascending=False).limit(top_n)\n",
    "\n",
    "    return recommendations.select(\"title\", \"similarity\").collect()\n",
    "\n",
    "# Example Usage\n",
    "query_title = \"A new approach of....\"\n",
    "recommended_papers = recommend_papers(query_title, top_n=5)\n",
    "print(f\"\\nRecommendations for '{query_title}':\\n\")\n",
    "for rec in recommended_papers:\n",
    "    print(f\"Title: {rec['title']}, Similarity: {rec['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28765e6b-dda5-480d-a3d4-a519a7f9b96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f8960-8db3-4649-9712-0ec1f624a64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
