{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed696d0-5905-4f84-ab70-da12c3bf3537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in /opt/conda/lib/python3.11/site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from langid) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "#Importing all\n",
    "!pip install langid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "import numpy as np\n",
    "import langid\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql import DataFrame\n",
    "# from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import VectorSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35293e97-8df2-4583-9248-ce50f76fa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Project 3\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b633be-3fc7-45ef-ba49-09953733b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52422)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Project 3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c479b8-f7ee-4040-a507-71555fa1ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dblp.v10.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -u dblp.v10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b412ae2-3ba2-41e9-9261-614b0e051383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)\n",
    "# df = spark.read.json(\"./dblp-ref/dblp-ref-0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af1a21e-484e-4297-a121-5b20ac3f4509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# List of file paths\n",
    "file_paths = [\"./dblp-ref/dblp-ref-0.json\", \"./dblp-ref/dblp-ref-1.json\", \"./dblp-ref/dblp-ref-2.json\", \"./dblp-ref/dblp-ref-3.json\"]\n",
    "\n",
    "# Read data from all JSON files\n",
    "dfs = [spark.read.json(file_path) for file_path in file_paths]\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "df = reduce(lambda df1, df2: df1.union(df2), dfs)\n",
    "\n",
    "# Show the combined DataFrame\n",
    "df.show(5)  # Show the first 5 rows to verify the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ce2893-0627-4e58-a576-d60c7cef35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of records: 3079007\n"
     ]
    }
   ],
   "source": [
    "# Show the schema\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Number of records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df179595-ce55-43bc-944f-effc8571d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+------------------+\n",
      "|summary|            abstract|                  id|        n_citation|                                title|               venue|              year|\n",
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+------------------+\n",
      "|  count|             2548532|             3079007|           3079007|                              3079007|             3079007|           3079007|\n",
      "|   mean|                NULL|                NULL|35.220902713114974|                                 NULL|                NULL|2007.7665994263734|\n",
      "| stddev|                NULL|                NULL|157.70065110545153|                                 NULL|                NULL| 7.816538498622617|\n",
      "|    min|\u000e\u000f\u0010\u0010\u0011 \u0012\u0013\u0010\u0014 \u000e\u000f\u0010\u0011 \u0012...|000000b8-7f59-49a...|                 0|                 ! and ? – Storage...|                    |              1936|\n",
      "|    max|𝑘�������-Anonymo...|ffffe1e6-981e-4cf...|             73362|利用雙語學術名詞庫抽取中英字詞互譯...|worst-case execut...|              2018|\n",
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+------------------+\n",
      "\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|abstract|authors| id|n_citation|references|title|venue|year|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "|  530475|      4|  0|         0|    362865|    0|    0|   0|\n",
      "+--------+-------+---+----------+----------+-----+-----+----+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|        n_citation|\n",
      "+-------+------------------+\n",
      "|  count|           3079007|\n",
      "|   mean|35.220902713114974|\n",
      "| stddev|157.70065110545153|\n",
      "|    min|                 0|\n",
      "|    max|             73362|\n",
      "+-------+------------------+\n",
      "\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|    NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|    NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "|    NULL|[Rafael Álvarez, ...|005ce28f-ed77-4e9...|         0|                NULL|COMPARING GNG3D A...|international con...|2009|\n",
      "|    NULL|[Jovan Dj. Golic,...|00638a94-23bf-4fa...|         2|                NULL|Vectorial fast co...|                    |2004|\n",
      "|    NULL|[Guzin Ulutas, Mu...|00701b05-684f-45f...|         0|[5626736c-e434-4e...|Improved Secret I...|international sym...|2011|\n",
      "|    NULL|[Pranay Chaudhuri...|00745041-3636-4d1...|         0|                NULL|A Self-Stabilizin...|parallel and dist...|2003|\n",
      "|    NULL|[Dominik Szajerma...|00964544-cbe2-4da...|         0|[3fcd7cdc-20e6-4e...|Fur Visualisation...|international con...|2014|\n",
      "|    NULL|[David Al-Dabass,...|00ba1eb4-d1aa-458...|         0|                NULL|Simulation of a v...|                    |1995|\n",
      "|    NULL|[Reiko Heckel, To...|00dc2bba-3237-4d4...|        50|                NULL|Software Evolutio...|Electronic Notes ...|2003|\n",
      "|    NULL|[Milos Zelezný, P...|00e02aeb-b424-4ca...|         7|                NULL|Design of an audi...|conference of the...|2002|\n",
      "|    NULL|[Abdullah Alsubai...|00e3940b-201b-40b...|         6|[0293844d-7244-45...|A Platform for Di...|                    |2013|\n",
      "|    NULL|[Tien Ho-Phuoc, A...|00fd4f6f-c7a1-49a...|         3|                NULL|A COMPUTATIONAL S...|international con...|2009|\n",
      "|    NULL|[Marco Baroni, Fr...|01082175-d4e3-456...|       101|                NULL|Cleaneval: a Comp...|language resource...|2008|\n",
      "|    NULL|[Nebojsa M. Ralev...|010fda73-e708-489...|         0|[17d92951-cd71-49...|Algorithms for th...|                    |2007|\n",
      "|    NULL|[William Perrizo,...|01103168-55d8-42b...|         0|                NULL|Vertical Database...|                    |2005|\n",
      "|    NULL|[Norihiko Moriwak...|014df191-abb2-44c...|         0|[08704b8d-70a1-44...|Sensor-Data-Drive...|                    |2014|\n",
      "|    NULL|[Prudence W. H. W...|01546472-0ac9-47f...|         6|[08784bb3-09b9-4b...|An 8/3 Lower Boun...|international sym...|2012|\n",
      "|    NULL|[Olivia Mendoza, ...|016eba4d-f912-492...|        50|[159fdfd2-921e-42...|Comparison of Fuz...|      soft computing|2010|\n",
      "|    NULL|[Martine Collard,...|01733dc2-4910-40d...|         3|[0102f234-e30b-4b...|Extracted Knowled...|research challeng...|2007|\n",
      "|    NULL|[Yosuke Furukawa,...|019a51de-5136-41c...|        50|                NULL|Context Dependent...|Journal of Advanc...|2007|\n",
      "+--------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Check for missing values (excluding `isnan`)\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Distribution of citations\n",
    "df.select(\"n_citation\").describe().show()\n",
    "\n",
    "# Check for null abstracts and titles\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7855f3f-7a7b-4d23-bf45-4c4981e11071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|language|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|      en|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|      en|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|      en|\n",
      "|Recent achievemen...|[Ankita Brahmacha...|00a119c4-d367-460...|         0|[84d47128-58d0-41...|Identifying Psych...|                    |2013|      en|\n",
      "|Recently, Bridges...|[Alvaro L. Islas,...|00bcf2d5-1592-46b...|        50|                  []|Multisymplectic S...|international con...|2002|      en|\n",
      "|Most applications...|[Patrick Cousot, ...|00c85316-bddf-4bc...|        50|[6e8a3ec3-9a99-4f...|Relational Abstra...|                    |1991|      en|\n",
      "|Three speech trai...|[Minoru Shigenaga...|00ca027b-5174-40f...|         0|                NULL|Speech training s...|international joi...|1979|      en|\n",
      "|This paper focuse...|[Efthymios Alepis...|00dd5ece-1339-4cb...|        50|[522bea13-bd45-49...|Knowledge Enginee...|joint conference ...|2008|      en|\n",
      "|Embedded systems ...|[Matias Madou, Bj...|01047814-b615-444...|        50|                NULL|Link-time compact...|european symposiu...|2004|      en|\n",
      "|Xax is a browser ...|[John R. Douceur,...|010d4ce9-0279-416...|        50|[0abc9de7-e047-44...|Leveraging legacy...|operating systems...|2008|      en|\n",
      "|In recent years, ...|[Hiroshi Furukawa...|010d9907-45ef-459...|         7|[b2f0e0d3-0071-40...|A pedestrian navi...|international con...|2013|      en|\n",
      "|Previous language...|[Carmen Fernández...|012b88ae-a763-45d...|        50|[00515e82-3da6-49...|Word pairs in lan...|                    |2004|      en|\n",
      "|Spatial encryptio...|[Michel Abdalla, ...|016a9a21-e882-4cd...|        50|[3140c9ba-8d98-42...|Leakage-Resilient...|                    |2012|      en|\n",
      "|In system operati...|[Mark Burgess, Al...|01705f09-d395-4a0...|         8|[363278e2-6c9d-45...|On system rollbac...|The Journal of Lo...|2011|      en|\n",
      "|Business strategy...|[Constantinos Gia...|01b6f2ca-3903-419...|        12|[0ce446cf-4f4f-49...|Model-Driven Stra...|                    |2012|      en|\n",
      "|FTP Mirror Tracke...|[Alexei Novikov, ...|01ccb92f-46f1-400...|         0|                  []|FTP Mirror Tracke...|usenix large inst...|2000|      en|\n",
      "|There are a numbe...|[Theresa Beauboue...|01edeac9-cd8b-46f...|         0|[09b666de-6279-48...|Information Syste...|                    |2014|      en|\n",
      "|Breast cancer is ...|[Bartosz Krawczyk...|0265aea8-65f3-4f4...|        50|[16f0341b-538d-40...|Breast Cancer Ide...|                    |2013|      en|\n",
      "|The development o...|[J. Niblock, Jian...|028d37c8-b571-41b...|         0|[8fc06656-ed2a-46...|Automated Object ...|international con...|2008|      en|\n",
      "|Quality specified...|[Qiong Liu, You Y...|02a5e8a8-061e-4c1...|         3|[04f8d077-765a-41...|Quality Assessmen...|conference on mul...|2013|      en|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to detect language using langid\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Add a new column for language detection\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "\n",
    "# Filter only English documents\n",
    "df = df.filter(df.language == 'en')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea38aff-6902-4413-87ae-bbe86f4ce128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', \n",
    "                     'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', \n",
    "                     'CZI', 'www']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea663933-fdbb-4ca7-9e30-7500a98a7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))\n",
    "# df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2d3f6f-b148-45ec-a1a4-713a229eb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "df_tokenized = df_cleaned.withColumn(\"tokenized_abstract\", split(col(\"cleaned_abstract\"), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb89221-56de-41ac-acfb-bfb28aa8ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokenized_abstract\", outputCol=\"filtered_abstract\", \n",
    "                           stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b340bef-4247-424e-8e9a-99568fdf5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the cleaned dataframe\n",
    "# df_filtered.select(\"abstract\", \"cleaned_abstract\", \"tokenized_abstract\", \"filtered_abstract\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b36a5a-d6c0-4e5a-8034-47d1b9886237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_abstract\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "df_featurized = hashingTF.transform(df_filtered)\n",
    "# df_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593dfa35-f111-431c-bb48-ad6b1dd73b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o193.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply IDF\u001b[39;00m\n\u001b[1;32m      2\u001b[0m idf \u001b[38;5;241m=\u001b[39m IDF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m idf_model \u001b[38;5;241m=\u001b[39m \u001b[43midf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_featurized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_vectorized \u001b[38;5;241m=\u001b[39m idf_model\u001b[38;5;241m.\u001b[39mtransform(df_featurized)\n\u001b[1;32m      5\u001b[0m df_vectorized\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o193.fit"
     ]
    }
   ],
   "source": [
    "# Apply IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_featurized)\n",
    "df_vectorized = idf_model.transform(df_featurized)\n",
    "df_vectorized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13acb9-a1c5-4ca9-9af2-2ee91133335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "df_vectorized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54347d4c-a4aa-44be-96a8-68852cf3bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering \n",
    "\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.count() == 0 and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.count() == 0:\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "# Sample the data with initial fraction 0.1\n",
    "df_sampled = sample_data(df_vectorized, 0.1)\n",
    "print(df_sampled)\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "df_sampled.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214f8c7-df05-4806-90cc-1db6cc8e089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Check if df_sampled is a Spark DataFrame\n",
    "# if isinstance(df_sampled, DataFrame):\n",
    "#     print(\"df_sampled is a Spark DataFrame\")\n",
    "# else:\n",
    "#     print(\"df_sampled is not a Spark DataFrame\")\n",
    "\n",
    "# # Check if df_sampled has a column named \"features\"\n",
    "# if \"features\" in df_sampled.columns:\n",
    "#     print(\"df_sampled has a column named 'features'\")\n",
    "# else:\n",
    "#     print(\"df_sampled does not have a column named 'features'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73aa44-69f3-481d-b38f-adfecf27d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VectorSlicer to select relevant features\n",
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"sliced_features\", indices=[i for i in range(0, 50)])\n",
    "df_sliced = slicer.transform(df_sampled)\n",
    "\n",
    "# Update PCA to use sliced features\n",
    "pca = PCA(k=2, inputCol=\"sliced_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Repartition the data to increase parallelism\n",
    "df_sampled_repartitioned = df_sliced.repartition(100)\n",
    "\n",
    "# Persist the DataFrame to speed up the computation\n",
    "df_sampled_repartitioned.persist()\n",
    "\n",
    "# Fit the PCA model\n",
    "pca_model = pca.fit(df_sampled_repartitioned)\n",
    "\n",
    "# Transform the data using the PCA model\n",
    "df_pca = pca_model.transform(df_sampled_repartitioned)\n",
    "\n",
    "# Show the result (for debugging purposes)\n",
    "df_pca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd85b3-edeb-402d-bd9d-18bfced22e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.select(\"pca_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec926e3-7280-41d3-97eb-ce1fadb4d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def vector_length(vector):\n",
    "    return len(vector)\n",
    "\n",
    "vector_length_udf = udf(vector_length, IntegerType())\n",
    "df_pca = df_pca.withColumn(\"pca_feature_length\", vector_length_udf(df_pca[\"pca_features\"]))\n",
    "df_pca.select(\"pca_feature_length\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289ef63-7603-44ca-81fc-52e919dc2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method to Find Optimal K\n",
    "costs = []\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = kmeans.fit(df_pca)\n",
    "    costs.append(model.summary.trainingCost)\n",
    "\n",
    "plt.plot(range(2, 21), costs, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af23dfb-5d97-43a7-9216-6a7aaf84b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KMeans Clustering (using the optimal k from the elbow method)\n",
    "# optimal_k = 10  # Example optimal k value\n",
    "# kmeans = KMeans(k=optimal_k, seed=1, featuresCol=\"pca_features\")\n",
    "# model = kmeans.fit(df_pca)\n",
    "# df_clustered = model.transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a17f10-e99e-4ae8-8980-5e41bd0e0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def recommend_papers(title, top_n=5):\n",
    "  # Assuming df_clustered is a pandas DataFrame\n",
    "  paper_cluster = df_clustered[df_clustered[\"title\"] == title][\"prediction\"].iloc[0]\n",
    "  cluster_papers = df_clustered[df_clustered[\"prediction\"] == paper_cluster]\n",
    "  paper_features = df_clustered[df_clustered[\"title\"] == title][\"pca_features\"].iloc[0]\n",
    "\n",
    "  similarities = []\n",
    "  for _, row in cluster_papers.iterrows():\n",
    "    other_title = row[\"Dynamic analysis\"]\n",
    "    other_features = row[\"pca_features\"]\n",
    "    similarity = cosine_similarity(paper_features, other_features)\n",
    "    similarities.append((other_title, similarity))\n",
    "\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28765e6b-dda5-480d-a3d4-a519a7f9b96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f8960-8db3-4649-9712-0ec1f624a64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
