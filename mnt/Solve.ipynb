{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed696d0-5905-4f84-ab70-da12c3bf3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all\n",
    "!pip install langid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "import numpy as np\n",
    "import langid\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql import DataFrame\n",
    "# from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import VectorSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35293e97-8df2-4583-9248-ce50f76fa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Project 3\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b633be-3fc7-45ef-ba49-09953733b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Project 3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c479b8-f7ee-4040-a507-71555fa1ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -u dblp.v10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b412ae2-3ba2-41e9-9261-614b0e051383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"./dblp-ref/*.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce2893-0627-4e58-a576-d60c7cef35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Number of records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df179595-ce55-43bc-944f-effc8571d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Check for missing values (excluding `isnan`)\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Distribution of citations\n",
    "df.select(\"n_citation\").describe().show()\n",
    "\n",
    "# Check for null abstracts and titles\n",
    "df.filter(df.abstract.isNull() | df.title.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7855f3f-7a7b-4d23-bf45-4c4981e11071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language using langid\n",
    "def detect_language(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "# Registering UDF\n",
    "lang_detect_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Add a new column for language detection\n",
    "df = df.withColumn(\"language\", lang_detect_udf(df.abstract))\n",
    "\n",
    "# Filter only English documents\n",
    "df = df.filter(df.language == 'en')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea38aff-6902-4413-87ae-bbe86f4ce128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', \n",
    "                     'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', \n",
    "                     'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', \n",
    "                     'CZI', 'www']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea663933-fdbb-4ca7-9e30-7500a98a7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove punctuation\n",
    "df_cleaned = df.withColumn(\"cleaned_abstract\", lower(col(\"abstract\")))\n",
    "df_cleaned = df_cleaned.withColumn(\"cleaned_abstract\", regexp_replace(col(\"cleaned_abstract\"), r'[!()\\-\\[\\]{};:\\'\",<>./?@#$%^&*_~]', ''))\n",
    "# df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d3f6f-b148-45ec-a1a4-713a229eb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "df_tokenized = df_cleaned.withColumn(\"tokenized_abstract\", split(col(\"cleaned_abstract\"), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb89221-56de-41ac-acfb-bfb28aa8ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokenized_abstract\", outputCol=\"filtered_abstract\", \n",
    "                           stopWords=StopWordsRemover().getStopWords() + custom_stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b340bef-4247-424e-8e9a-99568fdf5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the cleaned dataframe\n",
    "# df_filtered.select(\"abstract\", \"cleaned_abstract\", \"tokenized_abstract\", \"filtered_abstract\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b36a5a-d6c0-4e5a-8034-47d1b9886237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_abstract\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "df_featurized = hashingTF.transform(df_filtered)\n",
    "# df_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593dfa35-f111-431c-bb48-ad6b1dd73b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_featurized)\n",
    "df_vectorized = idf_model.transform(df_featurized)\n",
    "df_vectorized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13acb9-a1c5-4ca9-9af2-2ee91133335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns we need\n",
    "df_vectorized = df_vectorized.select(\"id\", \"title\", \"features\")\n",
    "df_vectorized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54347d4c-a4aa-44be-96a8-68852cf3bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering \n",
    "\n",
    "def sample_data(df, fraction, max_attempts=5):\n",
    "    attempt = 0\n",
    "    sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "    while sampled_df.count() == 0 and attempt < max_attempts:\n",
    "        fraction *= 2  # Increase fraction to get more data\n",
    "        sampled_df = df.sample(fraction=fraction, seed=42)\n",
    "        attempt += 1\n",
    "    if sampled_df.count() == 0:\n",
    "        raise ValueError(\"Sampled DataFrame is empty after several attempts.\")\n",
    "    return sampled_df\n",
    "\n",
    "# Sample the data with initial fraction 0.1\n",
    "df_sampled = sample_data(df_vectorized, 0.1)\n",
    "print(df_sampled)\n",
    "print(f\"Sampled DataFrame count: {df_sampled.count()}\")\n",
    "df_sampled.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214f8c7-df05-4806-90cc-1db6cc8e089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Check if df_sampled is a Spark DataFrame\n",
    "# if isinstance(df_sampled, DataFrame):\n",
    "#     print(\"df_sampled is a Spark DataFrame\")\n",
    "# else:\n",
    "#     print(\"df_sampled is not a Spark DataFrame\")\n",
    "\n",
    "# # Check if df_sampled has a column named \"features\"\n",
    "# if \"features\" in df_sampled.columns:\n",
    "#     print(\"df_sampled has a column named 'features'\")\n",
    "# else:\n",
    "#     print(\"df_sampled does not have a column named 'features'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73aa44-69f3-481d-b38f-adfecf27d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VectorSlicer to select relevant features\n",
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"sliced_features\", indices=[i for i in range(0, 50)])\n",
    "df_sliced = slicer.transform(df_sampled)\n",
    "\n",
    "# Update PCA to use sliced features\n",
    "pca = PCA(k=2, inputCol=\"sliced_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Repartition the data to increase parallelism\n",
    "df_sampled_repartitioned = df_sliced.repartition(100)\n",
    "\n",
    "# Persist the DataFrame to speed up the computation\n",
    "df_sampled_repartitioned.persist()\n",
    "\n",
    "# Fit the PCA model\n",
    "pca_model = pca.fit(df_sampled_repartitioned)\n",
    "\n",
    "# Transform the data using the PCA model\n",
    "df_pca = pca_model.transform(df_sampled_repartitioned)\n",
    "\n",
    "# Show the result (for debugging purposes)\n",
    "df_pca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd85b3-edeb-402d-bd9d-18bfced22e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.select(\"pca_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec926e3-7280-41d3-97eb-ce1fadb4d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def vector_length(vector):\n",
    "    return len(vector)\n",
    "\n",
    "vector_length_udf = udf(vector_length, IntegerType())\n",
    "df_pca = df_pca.withColumn(\"pca_feature_length\", vector_length_udf(df_pca[\"pca_features\"]))\n",
    "df_pca.select(\"pca_feature_length\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289ef63-7603-44ca-81fc-52e919dc2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method to Find Optimal K\n",
    "costs = []\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\")\n",
    "    model = kmeans.fit(df_pca)\n",
    "    costs.append(model.summary.trainingCost)\n",
    "\n",
    "plt.plot(range(2, 21), costs, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af23dfb-5d97-43a7-9216-6a7aaf84b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KMeans Clustering (using the optimal k from the elbow method)\n",
    "# optimal_k = 10  # Example optimal k value\n",
    "# kmeans = KMeans(k=optimal_k, seed=1, featuresCol=\"pca_features\")\n",
    "# model = kmeans.fit(df_pca)\n",
    "# df_clustered = model.transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a17f10-e99e-4ae8-8980-5e41bd0e0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def recommend_papers(title, top_n=5):\n",
    "  # Assuming df_clustered is a pandas DataFrame\n",
    "  paper_cluster = df_clustered[df_clustered[\"title\"] == title][\"prediction\"].iloc[0]\n",
    "  cluster_papers = df_clustered[df_clustered[\"prediction\"] == paper_cluster]\n",
    "  paper_features = df_clustered[df_clustered[\"title\"] == title][\"pca_features\"].iloc[0]\n",
    "\n",
    "  similarities = []\n",
    "  for _, row in cluster_papers.iterrows():\n",
    "    other_title = row[\"Dynamic analysis\"]\n",
    "    other_features = row[\"pca_features\"]\n",
    "    similarity = cosine_similarity(paper_features, other_features)\n",
    "    similarities.append((other_title, similarity))\n",
    "\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28765e6b-dda5-480d-a3d4-a519a7f9b96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f8960-8db3-4649-9712-0ec1f624a64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
